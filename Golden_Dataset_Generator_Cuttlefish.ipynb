{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golden Dataset Generator - Cuttlefish JIRA\n",
    "\n",
    "This notebook creates a synthetic golden dataset using RAGAS from JIRA issue data and uploads it to LangSmith.\n",
    "\n",
    "**Features:**\n",
    "- Self-contained (no dependencies on other notebooks)\n",
    "- Generates 15 synthetic Q&A pairs from JIRA issues\n",
    "- Uses \"description\" and \"title\" fields as input sources\n",
    "- Uploads dataset to LangSmith for evaluation tracking\n",
    "- Uses \"Abstracted SDG\" approach from RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q ragas langsmith langchain-community datasets pandas numpy pillow rapidfuzz\n",
    "!pip install -q langchain-openai langchain-core qdrant-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: API Keys Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API keys configured successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from uuid import uuid4\n",
    "\n",
    "# OpenAI API Key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# LangSmith API configuration\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "if \"LANGCHAIN_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key: \")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Cuttlefish - JIRA Golden Dataset Generation - {uuid4().hex[0:8]}\"\n",
    "\n",
    "print(\"âœ… API keys configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load JIRA Issue Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JIRA issue data...\n",
      "âœ… Loaded 100 JIRA issue documents\n",
      "Example document preview: Issue Title: MAX_VERSIONS not respected.\n",
      "\n",
      "Description: Below is a report from the list.  I confirmed playing in shell that indeed we have this problem.  Lets fix for 0.2.1.{code}Hello.I made some tests with HBase 0.2.0 (RC2), focused on insertion andtimestamps behaviour. I had some surprising result...\n",
      "\n",
      "Top 5 projects in sample:\n",
      "  HBASE: 100 issues\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set CSV field size limit for large descriptions\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "# Load JIRA data from CSV\n",
    "print(\"Loading JIRA issue data...\")\n",
    "jira_documents = []\n",
    "\n",
    "with open('./JIRA_OPEN_DATA_LARGESET_DATESHIFTED.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    \n",
    "    for i, row in enumerate(reader):\n",
    "        # Combine title and description for rich content\n",
    "        title = row.get('title', '').strip()\n",
    "        description = row.get('description', '').strip()\n",
    "        \n",
    "        # Skip empty entries\n",
    "        if not title and not description:\n",
    "            continue\n",
    "            \n",
    "        # Create combined content with title and description\n",
    "        if title and description:\n",
    "            content = f\"Issue Title: {title}\\n\\nDescription: {description}\"\n",
    "        elif title:\n",
    "            content = f\"Issue Title: {title}\"\n",
    "        else:\n",
    "            content = f\"Description: {description}\"\n",
    "        \n",
    "        # Create document with metadata\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"key\": row.get('key', ''),\n",
    "                \"project\": row.get('project', ''),\n",
    "                \"project_name\": row.get('project_name', ''),\n",
    "                \"priority\": row.get('priority', ''),\n",
    "                \"type\": row.get('type', ''),\n",
    "                \"status\": row.get('status', ''),\n",
    "                \"created\": row.get('created', ''),\n",
    "                \"title\": title,\n",
    "                \"description_length\": len(description)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        jira_documents.append(doc)\n",
    "        \n",
    "        # Limit to first 100 documents for manageable processing\n",
    "        if len(jira_documents) >= 100:\n",
    "            break\n",
    "\n",
    "print(f\"âœ… Loaded {len(jira_documents)} JIRA issue documents\")\n",
    "print(f\"Example document preview: {jira_documents[0].page_content[:300]}...\")\n",
    "\n",
    "# Show project distribution\n",
    "projects = [doc.metadata['project'] for doc in jira_documents]\n",
    "project_counts = pd.Series(projects).value_counts().head(5)\n",
    "print(f\"\\nTop 5 projects in sample:\")\n",
    "for project, count in project_counts.items():\n",
    "    print(f\"  {project}: {count} issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Synthetic Data Generation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/foohm/github/cuttlefish3/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Synthetic data generation models configured successfully!\n",
      "Using GPT-4o-mini for better technical content understanding\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Set up the LLM and embedding models for synthetic data generation\n",
    "# Using GPT-4 for better understanding of technical JIRA content\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "print(\"âœ… Synthetic data generation models configured successfully!\")\n",
    "print(\"Using GPT-4o-mini for better technical content understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Synthetic Golden Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic dataset using RAGAS...\n",
      "Using first 50 documents from 100 total JIRA issue documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying CustomNodeFilter:   0%|          | 0/50 [00:00<?, ?it/s]         Node cdc91764-06db-49fb-ba8f-d3bff675f972 does not have a summary. Skipping filtering.\n",
      "Node 9f03bac9-613b-4132-bb68-3b299aeeac58 does not have a summary. Skipping filtering.\n",
      "Node b6b877c5-8237-401e-aabe-3f420f97a5c8 does not have a summary. Skipping filtering.\n",
      "Node f4c3e8f9-792f-4bbe-9d6f-935fd4f94014 does not have a summary. Skipping filtering.\n",
      "Node 146f9f7a-e6a5-4ee2-b3fd-9d5efb56f4c2 does not have a summary. Skipping filtering.\n",
      "Node 399f46ee-6442-48d9-9d8e-99a11566d90c does not have a summary. Skipping filtering.\n",
      "Node 5ce4c6f0-dfaf-41d6-bc82-34d395c7f840 does not have a summary. Skipping filtering.\n",
      "Node f29037d1-c266-4ecc-b742-603085814bd8 does not have a summary. Skipping filtering.\n",
      "Node fca28fa9-aaa5-452f-8148-b746b2f921e7 does not have a summary. Skipping filtering.\n",
      "Node 78d5b51a-bec3-439d-8bce-4fe048b3af41 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  20%|â–ˆâ–ˆ        | 10/50 [00:00<00:02, 18.97it/s]Node d8e7868b-8084-4d8d-9fb8-1a3b9b6c6afb does not have a summary. Skipping filtering.\n",
      "Node f609f07c-d2dc-4a03-92c2-f4a95a764a15 does not have a summary. Skipping filtering.\n",
      "Node 762c5fc9-d2a6-4354-a1ad-55addf404846 does not have a summary. Skipping filtering.\n",
      "Node e3b13d9f-3daa-4eb2-8bec-1338d9435677 does not have a summary. Skipping filtering.\n",
      "Node 6486ac02-5766-4594-81cb-0bd89f690a93 does not have a summary. Skipping filtering.\n",
      "Node c7f2a437-8370-4ac7-8cee-b97755c3bc91 does not have a summary. Skipping filtering.\n",
      "Node d6407380-1cc9-45f0-886f-4d16993f2db6 does not have a summary. Skipping filtering.\n",
      "Node 3b326a63-65f5-4b1d-8769-c307ced46f20 does not have a summary. Skipping filtering.\n",
      "Applying CustomNodeFilter:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:00<00:00, 43.49it/s]Node 294f7021-6f57-48cf-b1e4-03490c67edee does not have a summary. Skipping filtering.\n",
      "Node 5cccbbfc-22f7-4e9a-b43d-30d92ab726d3 does not have a summary. Skipping filtering.\n",
      "Node f2f2eae4-2737-4215-ab98-8aa1de8bd359 does not have a summary. Skipping filtering.\n",
      "Node 74a75599-0f3b-40af-9dac-f0c350b38ea7 does not have a summary. Skipping filtering.\n",
      "Generating personas: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  2.25it/s]                                             \n",
      "Generating Scenarios: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.92s/it]\n",
      "Generating Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:04<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 15 synthetic Q&A pairs successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic dataset using the abstracted SDG approach\n",
    "print(\"Generating synthetic dataset using RAGAS...\")\n",
    "print(f\"Using first 50 documents from {len(jira_documents)} total JIRA issue documents\")\n",
    "\n",
    "# Initialize the test set generator\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "\n",
    "# Generate 15 synthetic Q&A pairs focused on JIRA technical issues\n",
    "synthetic_dataset = generator.generate_with_langchain_docs(\n",
    "    jira_documents[:50],  # Use first 50 docs to balance quality and speed\n",
    "    testset_size=15  # Generate exactly 15 Q&A pairs as requested\n",
    ")\n",
    "\n",
    "print(f\"âœ… Generated {len(synthetic_dataset)} synthetic Q&A pairs successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Convert to Pandas and Display Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden dataset shape: (15, 4)\n",
      "Columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']\n",
      "\n",
      "================================================================================\n",
      "JIRA GOLDEN DATASET PREVIEW:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the issue with HBase regarding the MAX...</td>\n",
       "      <td>[Issue Title: MAX_VERSIONS not respected.\\n\\nD...</td>\n",
       "      <td>The issue reported is that the MAX_VERSIONS pa...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What issues are associated with the node at IP...</td>\n",
       "      <td>[Issue Title: Splitting log in a hostile envir...</td>\n",
       "      <td>The node at IP address 10.252.219.207 is exper...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wut is the problem with DemoClient.java?</td>\n",
       "      <td>[Issue Title: Thrift host and port are hardcod...</td>\n",
       "      <td>The problem with DemoClient.java is that the T...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What is the issue with HBase regarding the MAX...   \n",
       "1  What issues are associated with the node at IP...   \n",
       "2           Wut is the problem with DemoClient.java?   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Issue Title: MAX_VERSIONS not respected.\\n\\nD...   \n",
       "1  [Issue Title: Splitting log in a hostile envir...   \n",
       "2  [Issue Title: Thrift host and port are hardcod...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The issue reported is that the MAX_VERSIONS pa...   \n",
       "1  The node at IP address 10.252.219.207 is exper...   \n",
       "2  The problem with DemoClient.java is that the T...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE JIRA QUESTION-ANSWER PAIR:\n",
      "================================================================================\n",
      "Question: What is the issue with HBase regarding the MAX_VERSIONS parameter and how does it affect data storage?\n",
      "\n",
      "Expected Answer: The issue reported is that the MAX_VERSIONS parameter is not being respected in HBase, leading to the conclusion that despite setting the VERSIONS parameter of the columns to 3, it appears that all versions of the data are being stored. This raises the question of whether there is a garbage collector process that removes old versions and, if so, when this process occurs.\n",
      "\n",
      "Reference Contexts (1 chunks):\n",
      "  Context 1: Issue Title: MAX_VERSIONS not respected.\n",
      "\n",
      "Description: Below is a report from the list.  I confirmed playing in shell that indeed we have this problem.  Lets fix for 0.2.1.{code}Hello.I made some tests with HBase 0.2.0 (RC2), focused on insertion and...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the synthetic dataset to pandas DataFrame\n",
    "golden_df = synthetic_dataset.to_pandas()\n",
    "\n",
    "print(f\"Golden dataset shape: {golden_df.shape}\")\n",
    "print(f\"Columns: {list(golden_df.columns)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"JIRA GOLDEN DATASET PREVIEW:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display first few examples\n",
    "display(golden_df.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE JIRA QUESTION-ANSWER PAIR:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show a detailed example\n",
    "example_idx = 0\n",
    "print(f\"Question: {golden_df.iloc[example_idx]['user_input']}\")\n",
    "print(f\"\\nExpected Answer: {golden_df.iloc[example_idx]['reference']}\")\n",
    "print(f\"\\nReference Contexts ({len(golden_df.iloc[example_idx]['reference_contexts'])} chunks):\")\n",
    "for i, context in enumerate(golden_df.iloc[example_idx]['reference_contexts'][:2]):  # Show first 2 contexts\n",
    "    print(f\"  Context {i+1}: {context[:250]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the issue with HBase regarding the MAX...</td>\n",
       "      <td>[Issue Title: MAX_VERSIONS not respected.\\n\\nD...</td>\n",
       "      <td>The issue reported is that the MAX_VERSIONS pa...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What issues are associated with the node at IP...</td>\n",
       "      <td>[Issue Title: Splitting log in a hostile envir...</td>\n",
       "      <td>The node at IP address 10.252.219.207 is exper...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wut is the problem with DemoClient.java?</td>\n",
       "      <td>[Issue Title: Thrift host and port are hardcod...</td>\n",
       "      <td>The problem with DemoClient.java is that the T...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the class org.apache.maven.surefire.j...</td>\n",
       "      <td>[Issue Title: MapReduce based tests broken on ...</td>\n",
       "      <td>The class org.apache.maven.surefire.junit4.JUn...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What compatibility issue arises in version 0.9...</td>\n",
       "      <td>[Issue Title: 0.94: HBASE-9865 breaks coproces...</td>\n",
       "      <td>In version 0.94, the change from 'public List&lt;...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What issues arise from the Short-Circuit Copro...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIssue Title: Short-Circuit Coproce...</td>\n",
       "      <td>The Short-Circuit Coprocessor not correctly lo...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What issues are related to backporting in the ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIssue Title: Backport HBASE-3890 '...</td>\n",
       "      <td>The issues related to backporting in the HBase...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What issues are associated with backporting HB...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIssue Title: Backport HBASE-3890 '...</td>\n",
       "      <td>The issues associated with backporting HBASE-3...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Why does the snapshot restoration process some...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIssue Title: MAX_VERSIONS not resp...</td>\n",
       "      <td>The snapshot restoration process sometimes fai...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the implications of the RegionTooBusy...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIssue Title: RegionTooBusyExceptio...</td>\n",
       "      <td>The RegionTooBusyException indicates that a re...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What issues arise when restoring snapshots for...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIssue Title: Restore snapshot fail...</td>\n",
       "      <td>When restoring snapshots for tablefour, there ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What issues arise from the corrupt HFile and h...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIssue Title: Corrupt HFile cause r...</td>\n",
       "      <td>The corrupt HFile issue leads to a resource le...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What are the performance implications of exces...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIssue Title: Excessive readpoint c...</td>\n",
       "      <td>The performance implications of excessive read...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What issues arise from the custom implementati...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIssue Title: Not starting Replicat...</td>\n",
       "      <td>The custom implementation of ReplicationSink n...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Why does the NullPointerException occur in Met...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIssue Title: Snapshot restore may ...</td>\n",
       "      <td>The NullPointerException occurs in MetaEditor ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What is the issue with HBase regarding the MAX...   \n",
       "1   What issues are associated with the node at IP...   \n",
       "2            Wut is the problem with DemoClient.java?   \n",
       "3   How does the class org.apache.maven.surefire.j...   \n",
       "4   What compatibility issue arises in version 0.9...   \n",
       "5   What issues arise from the Short-Circuit Copro...   \n",
       "6   What issues are related to backporting in the ...   \n",
       "7   What issues are associated with backporting HB...   \n",
       "8   Why does the snapshot restoration process some...   \n",
       "9   What are the implications of the RegionTooBusy...   \n",
       "10  What issues arise when restoring snapshots for...   \n",
       "11  What issues arise from the corrupt HFile and h...   \n",
       "12  What are the performance implications of exces...   \n",
       "13  What issues arise from the custom implementati...   \n",
       "14  Why does the NullPointerException occur in Met...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [Issue Title: MAX_VERSIONS not respected.\\n\\nD...   \n",
       "1   [Issue Title: Splitting log in a hostile envir...   \n",
       "2   [Issue Title: Thrift host and port are hardcod...   \n",
       "3   [Issue Title: MapReduce based tests broken on ...   \n",
       "4   [Issue Title: 0.94: HBASE-9865 breaks coproces...   \n",
       "5   [<1-hop>\\n\\nIssue Title: Short-Circuit Coproce...   \n",
       "6   [<1-hop>\\n\\nIssue Title: Backport HBASE-3890 '...   \n",
       "7   [<1-hop>\\n\\nIssue Title: Backport HBASE-3890 '...   \n",
       "8   [<1-hop>\\n\\nIssue Title: MAX_VERSIONS not resp...   \n",
       "9   [<1-hop>\\n\\nIssue Title: RegionTooBusyExceptio...   \n",
       "10  [<1-hop>\\n\\nIssue Title: Restore snapshot fail...   \n",
       "11  [<1-hop>\\n\\nIssue Title: Corrupt HFile cause r...   \n",
       "12  [<1-hop>\\n\\nIssue Title: Excessive readpoint c...   \n",
       "13  [<1-hop>\\n\\nIssue Title: Not starting Replicat...   \n",
       "14  [<1-hop>\\n\\nIssue Title: Snapshot restore may ...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   The issue reported is that the MAX_VERSIONS pa...   \n",
       "1   The node at IP address 10.252.219.207 is exper...   \n",
       "2   The problem with DemoClient.java is that the T...   \n",
       "3   The class org.apache.maven.surefire.junit4.JUn...   \n",
       "4   In version 0.94, the change from 'public List<...   \n",
       "5   The Short-Circuit Coprocessor not correctly lo...   \n",
       "6   The issues related to backporting in the HBase...   \n",
       "7   The issues associated with backporting HBASE-3...   \n",
       "8   The snapshot restoration process sometimes fai...   \n",
       "9   The RegionTooBusyException indicates that a re...   \n",
       "10  When restoring snapshots for tablefour, there ...   \n",
       "11  The corrupt HFile issue leads to a resource le...   \n",
       "12  The performance implications of excessive read...   \n",
       "13  The custom implementation of ReplicationSink n...   \n",
       "14  The NullPointerException occurs in MetaEditor ...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   single_hop_specifc_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_abstract_query_synthesizer  \n",
       "9   multi_hop_abstract_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  \n",
       "11  multi_hop_specific_query_synthesizer  \n",
       "12  multi_hop_specific_query_synthesizer  \n",
       "13  multi_hop_specific_query_synthesizer  \n",
       "14  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Upload Dataset to LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created LangSmith dataset: cuttlefish-jira-golden-dataset-20250731-122634\n",
      "Dataset ID: cb6d3deb-3b08-4b8e-8270-3eb5de4d328e\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Create a unique dataset name with timestamp\n",
    "dataset_name = f\"cuttlefish-jira-golden-dataset-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "# Create the dataset in LangSmith\n",
    "try:\n",
    "    langsmith_dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Cuttlefish golden dataset for RAG evaluation using 15 synthetic JIRA issue Q&A pairs generated with RAGAS from title and description fields\"\n",
    "    )\n",
    "    print(f\"âœ… Created LangSmith dataset: {dataset_name}\")\n",
    "    print(f\"Dataset ID: {langsmith_dataset.id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataset: {e}\")\n",
    "    # If dataset already exists, get it\n",
    "    langsmith_dataset = client.read_dataset(dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading JIRA examples to LangSmith...\n",
      "âœ… Successfully uploaded 15/15 examples to LangSmith dataset!\n"
     ]
    }
   ],
   "source": [
    "# Upload examples to LangSmith dataset\n",
    "print(\"Uploading JIRA examples to LangSmith...\")\n",
    "\n",
    "upload_count = 0\n",
    "for idx, row in golden_df.iterrows():\n",
    "    try:\n",
    "        client.create_example(\n",
    "            inputs={\n",
    "                \"question\": row[\"user_input\"]\n",
    "            },\n",
    "            outputs={\n",
    "                \"answer\": row[\"reference\"]\n",
    "            },\n",
    "            metadata={\n",
    "                \"reference_contexts\": row[\"reference_contexts\"],\n",
    "                \"synthesizer_name\": row.get(\"synthesizer_name\", \"unknown\"),\n",
    "                \"evolution_type\": row.get(\"evolution_type\", \"unknown\"),\n",
    "                \"episode_done\": row.get(\"episode_done\", False),\n",
    "                \"dataset_size\": len(golden_df),\n",
    "                \"source\": \"cuttlefish_jira_generator\",\n",
    "                \"data_source\": \"JIRA_OPEN_DATA_LARGESET_DATESHIFTED.csv\",\n",
    "                \"content_fields\": \"title + description\"\n",
    "            },\n",
    "            dataset_id=langsmith_dataset.id\n",
    "        )\n",
    "        upload_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading example {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"âœ… Successfully uploaded {upload_count}/{len(golden_df)} examples to LangSmith dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CUTTLEFISH JIRA GOLDEN DATASET GENERATION COMPLETE!\n",
      "================================================================================\n",
      "ðŸ“Š Dataset Name: cuttlefish-jira-golden-dataset-20250731-122634\n",
      "ðŸ“Š Dataset ID: cb6d3deb-3b08-4b8e-8270-3eb5de4d328e\n",
      "ðŸ“Š Number of Q&A Pairs: 15\n",
      "ðŸ“Š Source Documents Used: 100 (first 50 for generation)\n",
      "ðŸ“Š Data Source: JIRA_OPEN_DATA_LARGESET_DATESHIFTED.csv\n",
      "ðŸ“Š Content Fields: title + description\n",
      "ðŸ“Š Generation Method: RAGAS Abstracted SDG\n",
      "ðŸ“Š Models Used:\n",
      "   - LLM: gpt-4o-mini\n",
      "   - Embeddings: text-embedding-3-small\n",
      "\n",
      "ðŸŽ¯ READY FOR JIRA RAG EVALUATION!\n",
      "This dataset can now be used to evaluate RAG chains focused on JIRA issue resolution.\n",
      "\n",
      "ðŸ“‹ Dataset Structure:\n",
      "   - Questions: 15 unique\n",
      "   - Answer Length: 578 chars avg\n",
      "   - Context Chunks: 1.7 per question\n",
      "\n",
      "ðŸ“ˆ Synthesizer Distribution:\n",
      "   - single_hop_specifc_query_synthesizer: 5 questions\n",
      "   - multi_hop_abstract_query_synthesizer: 5 questions\n",
      "   - multi_hop_specific_query_synthesizer: 5 questions\n",
      "\n",
      "ðŸ”§ JIRA Content Analysis:\n",
      "   - Technical Questions: 14/15\n",
      "   - Average Question Length: 129 chars\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CUTTLEFISH JIRA GOLDEN DATASET GENERATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ðŸ“Š Dataset Name: {dataset_name}\")\n",
    "print(f\"ðŸ“Š Dataset ID: {langsmith_dataset.id}\")\n",
    "print(f\"ðŸ“Š Number of Q&A Pairs: {len(golden_df)}\")\n",
    "print(f\"ðŸ“Š Source Documents Used: {len(jira_documents)} (first 50 for generation)\")\n",
    "print(f\"ðŸ“Š Data Source: JIRA_OPEN_DATA_LARGESET_DATESHIFTED.csv\")\n",
    "print(f\"ðŸ“Š Content Fields: title + description\")\n",
    "print(f\"ðŸ“Š Generation Method: RAGAS Abstracted SDG\")\n",
    "print(f\"ðŸ“Š Models Used:\")\n",
    "print(f\"   - LLM: gpt-4o-mini\")\n",
    "print(f\"   - Embeddings: text-embedding-3-small\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ READY FOR JIRA RAG EVALUATION!\")\n",
    "print(\"This dataset can now be used to evaluate RAG chains focused on JIRA issue resolution.\")\n",
    "print(\"\\nðŸ“‹ Dataset Structure:\")\n",
    "print(f\"   - Questions: {golden_df['user_input'].nunique()} unique\")\n",
    "print(f\"   - Answer Length: {golden_df['reference'].str.len().mean():.0f} chars avg\")\n",
    "print(f\"   - Context Chunks: {golden_df['reference_contexts'].apply(len).mean():.1f} per question\")\n",
    "\n",
    "# Display synthesizer distribution\n",
    "if 'synthesizer_name' in golden_df.columns:\n",
    "    synthesizer_counts = golden_df['synthesizer_name'].value_counts()\n",
    "    print(f\"\\nðŸ“ˆ Synthesizer Distribution:\")\n",
    "    for synthesizer, count in synthesizer_counts.items():\n",
    "        print(f\"   - {synthesizer}: {count} questions\")\n",
    "\n",
    "print(\"\\nðŸ”§ JIRA Content Analysis:\")\n",
    "# Analyze question types\n",
    "questions = golden_df['user_input'].tolist()\n",
    "technical_keywords = ['error', 'exception', 'bug', 'fix', 'issue', 'problem', 'failure']\n",
    "technical_questions = sum(1 for q in questions if any(keyword in q.lower() for keyword in technical_keywords))\n",
    "print(f\"   - Technical Questions: {technical_questions}/{len(questions)}\")\n",
    "print(f\"   - Average Question Length: {golden_df['user_input'].str.len().mean():.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Save Dataset Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Dataset saved locally as: cuttlefish_jira_golden_dataset_20250731_122806.csv\n",
      "ðŸ’¾ Dataset saved locally as JSON: cuttlefish_jira_golden_dataset_20250731_122806.json\n",
      "\n",
      "ðŸ“ Files created:\n",
      "   - CSV: cuttlefish_jira_golden_dataset_20250731_122806.csv\n",
      "   - JSON: cuttlefish_jira_golden_dataset_20250731_122806.json\n",
      "   - LangSmith Dataset: cuttlefish-jira-golden-dataset-20250731-122634\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset locally for backup/reference\n",
    "local_filename = f\"cuttlefish_jira_golden_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "golden_df.to_csv(local_filename, index=False)\n",
    "print(f\"ðŸ’¾ Dataset saved locally as: {local_filename}\")\n",
    "\n",
    "# Also save as JSON for better context preservation\n",
    "json_filename = f\"cuttlefish_jira_golden_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "golden_df.to_json(json_filename, orient='records', indent=2)\n",
    "print(f\"ðŸ’¾ Dataset saved locally as JSON: {json_filename}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Files created:\")\n",
    "print(f\"   - CSV: {local_filename}\")\n",
    "print(f\"   - JSON: {json_filename}\")\n",
    "print(f\"   - LangSmith Dataset: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUALITY CHECK: ALL GENERATED QUESTIONS\n",
      "================================================================================\n",
      " 1. What is the issue with HBase regarding the MAX_VERSIONS parameter and how does it affect data storage?\n",
      " 2. What issues are associated with the node at IP address 10.252.219.207 during the log splitting process?\n",
      " 3. Wut is the problem with DemoClient.java?\n",
      " 4. How does the class org.apache.maven.surefire.junit4.JUnit4TestSet relate to the errors encountered in MapReduce tests on Hadoop 2.0.0-alpha?\n",
      " 5. What compatibility issue arises in version 0.94 related to HBASE-9865?\n",
      " 6. What issues arise from the Short-Circuit Coprocessor not correctly looking up tables on the server, and how does this relate to the problems caused by a corrupt HFile leading to resource leaks and OOM errors in region servers?\n",
      " 7. What issues are related to backporting in the HBase cluster and how do they affect connection to the new active master?\n",
      " 8. What issues are associated with backporting HBASE-3890 to version 0.94 and how do they relate to the sporadic failures encountered during snapshot restoration?\n",
      " 9. Why does the snapshot restoration process sometimes fail to find the table in meta after restoring a snapshot, and how does this relate to the MAX_VERSIONS issue in HBase?\n",
      "10. What are the implications of the RegionTooBusyException in relation to connection issues when switching to a new active master in an HBase cluster?\n",
      "11. What issues arise when restoring snapshots for tablefour and how does it relate to data loss in cloned tables?\n",
      "12. What issues arise from the corrupt HFile and how does it relate to the logging of coprocessor exec calls in HMaster?\n",
      "13. What are the performance implications of excessive readpoint checks in MemStoreScanner and how do they compare to those in StoreFileScanner?\n",
      "14. What issues arise from the custom implementation of ReplicationSink and how do they relate to the memory problems in ReplicationSource?\n",
      "15. Why does the NullPointerException occur in MetaEditor when trying to restore a snapshot, and how does it relate to the sporadic failures in restoring meta edits?\n",
      "\n",
      "================================================================================\n",
      "CONTEXT ANALYSIS\n",
      "================================================================================\n",
      "Total context chunks used: 25\n",
      "Average context length: 2447 chars\n",
      "Contexts with JIRA-specific terms: 16/25 (64.0%)\n"
     ]
    }
   ],
   "source": [
    "# Quality check: Display all questions to review variety and quality\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUALITY CHECK: ALL GENERATED QUESTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(golden_df['user_input'], 1):\n",
    "    print(f\"{i:2d}. {question}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONTEXT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze context sources\n",
    "all_contexts = []\n",
    "for contexts in golden_df['reference_contexts']:\n",
    "    all_contexts.extend(contexts)\n",
    "\n",
    "print(f\"Total context chunks used: {len(all_contexts)}\")\n",
    "print(f\"Average context length: {sum(len(ctx) for ctx in all_contexts) / len(all_contexts):.0f} chars\")\n",
    "\n",
    "# Check for JIRA-specific content in contexts\n",
    "jira_terms = ['HBASE', 'FLEX', 'JBIDE', 'SPR', 'exception', 'error', 'bug', 'fix']\n",
    "jira_contexts = sum(1 for ctx in all_contexts if any(term in ctx for term in jira_terms))\n",
    "print(f\"Contexts with JIRA-specific terms: {jira_contexts}/{len(all_contexts)} ({jira_contexts/len(all_contexts)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
