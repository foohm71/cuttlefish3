{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-IqJAMkwnCF"
   },
   "source": [
    "# Advanced Retrieval with LangChain\n",
    "\n",
    "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
    "\n",
    "We'll touch on:\n",
    "\n",
    "- Naive Retrieval\n",
    "- Best-Matching 25 (BM25)\n",
    "- Multi-Query Retrieval\n",
    "- Parent-Document Retrieval\n",
    "- Contextual Compression (a.k.a. Rerank)\n",
    "- Ensemble Retrieval\n",
    "- Semantic chunking\n",
    "\n",
    "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
    "\n",
    "There will be two breakout rooms:\n",
    "\n",
    "- ðŸ¤ Breakout Room Part #1\n",
    "  - Task 1: Getting Dependencies!\n",
    "  - Task 2: Data Collection and Preparation\n",
    "  - Task 3: Setting Up QDrant!\n",
    "  - Task 4-10: Retrieval Strategies\n",
    "- ðŸ¤ Breakout Room Part #2\n",
    "  - Activity: Evaluate with Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rKP3hgHivpe"
   },
   "source": [
    "# ðŸ¤ Breakout Room Part #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xes8oT-xHN7"
   },
   "source": [
    "## Task 1: Getting Dependencies!\n",
    "\n",
    "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7OHJXzfyJyA"
   },
   "source": [
    "We'll also provide our OpenAI key, as well as our Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LttlDQUYgSI",
    "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iUahNiJyQbv",
    "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
   },
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw304iAFyRtl"
   },
   "source": [
    "## Task 2: Data Collection and Preparation\n",
    "\n",
    "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A92NC2QZzCsi"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "GshBjVRJZ6p8"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=f\"./data/complaints.csv\",\n",
    "    metadata_columns=[\n",
    "      \"Date received\", \n",
    "      \"Product\", \n",
    "      \"Sub-product\", \n",
    "      \"Issue\", \n",
    "      \"Sub-issue\", \n",
    "      \"Consumer complaint narrative\", \n",
    "      \"Company public response\", \n",
    "      \"Company\", \n",
    "      \"State\", \n",
    "      \"ZIP code\", \n",
    "      \"Tags\", \n",
    "      \"Consumer consent provided?\", \n",
    "      \"Submitted via\", \n",
    "      \"Date sent to company\", \n",
    "      \"Company response to consumer\", \n",
    "      \"Timely response?\", \n",
    "      \"Consumer disputed?\", \n",
    "      \"Complaint ID\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "loan_complaint_data = loader.load()\n",
    "\n",
    "for doc in loan_complaint_data:\n",
    "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gQphb6y0C0S"
   },
   "source": [
    "Let's look at an example document to see if everything worked as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkUkCf7DaMiq",
    "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/complaints.csv', 'row': 1, 'Date received': '05/09/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Received bad information about your loan', 'Consumer complaint narrative': 'I submitted my annual Income-Driven Repayment ( IDR ) recertification to Aidvantage on time and in full, using the correct process. My income is approximately $ XXXX/year, which is below 150 % of the federal poverty guideline. Under the SAVE Plan or any valid IDR/IBR plan , this income level qualifies for a $ XXXX monthly payment. \\n\\nInstead, Aidvantage assigned me a $ XXXX/month paymentan amount that is not legally or mathematically possible based on my income. I contacted them and was placed in a two-month administrative forbearance. After that, they returned with the exact same incorrect payment amount. \\n\\nWhen I pressed for clarification, Aidvantage responded that my IDR application has not been processed yet, even though they had already issued a bill and claimed it was based on a recalculation. This is a clear contradiction. You can not bill a borrower for an IDR plan amount before processing the application. \\n\\nAidvantage has not requested any additional documentation from me. There has been no communication asking for clarification or income verification beyond my original submission. \\n\\nI am requesting : A correction of my repayment amount in accordance with my documented income ; A written explanation of how the erroneous amount was calculated ; A guarantee that no delinquency or negative credit reporting occurs while this dispute is unresolved ; An investigation into why borrowers are being billed despite applications not being processed.', 'Company public response': 'None', 'Company': 'Maximus Federal Services, Inc.', 'State': 'VA', 'ZIP code': '230XX', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '05/09/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '13425612'}, page_content='I submitted my annual Income-Driven Repayment ( IDR ) recertification to Aidvantage on time and in full, using the correct process. My income is approximately $ XXXX/year, which is below 150 % of the federal poverty guideline. Under the SAVE Plan or any valid IDR/IBR plan , this income level qualifies for a $ XXXX monthly payment. \\n\\nInstead, Aidvantage assigned me a $ XXXX/month paymentan amount that is not legally or mathematically possible based on my income. I contacted them and was placed in a two-month administrative forbearance. After that, they returned with the exact same incorrect payment amount. \\n\\nWhen I pressed for clarification, Aidvantage responded that my IDR application has not been processed yet, even though they had already issued a bill and claimed it was based on a recalculation. This is a clear contradiction. You can not bill a borrower for an IDR plan amount before processing the application. \\n\\nAidvantage has not requested any additional documentation from me. There has been no communication asking for clarification or income verification beyond my original submission. \\n\\nI am requesting : A correction of my repayment amount in accordance with my documented income ; A written explanation of how the erroneous amount was calculated ; A guarantee that no delinquency or negative credit reporting occurs while this dispute is unresolved ; An investigation into why borrowers are being billed despite applications not being processed.')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_complaint_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWaQpdHl0Gzc"
   },
   "source": [
    "## Task 3: Setting up QDrant!\n",
    "\n",
    "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
    "\n",
    "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
    "\n",
    "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "NT8ihRJbYmMT"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    loan_complaint_data,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LoanComplaints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x2SS4Rh0hiN"
   },
   "source": [
    "## Task 4: Naive RAG Chain\n",
    "\n",
    "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEH7X5Ai08FH"
   },
   "source": [
    "### R - Retrieval\n",
    "\n",
    "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
    "\n",
    "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "GFDPrNBtb72o"
   },
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbBhyQjz06dx"
   },
   "source": [
    "### A - Augmented\n",
    "\n",
    "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "7uSz-Dbqcoki"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlRzpb231GGJ"
   },
   "source": [
    "### G - Generation\n",
    "\n",
    "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "c-1t9H60dJLg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg3QRGzA1M2x"
   },
   "source": [
    "### LCEL RAG Chain\n",
    "\n",
    "We're going to use LCEL to construct our chain.\n",
    "\n",
    "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "0bvstS7mdOW3"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izKujhNb1ZG8"
   },
   "source": [
    "Let's see how this simple chain does on a few different prompts.\n",
    "\n",
    "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LI-5ueEddku9",
    "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, the most common issues with loans appear to be:\\n\\n- Dealing with lenders or servicers, including receiving bad or incorrect information about the loan, and issues related to loan balances, misapplied payments, and wrongful denials of payment plans.\\n- Problems with payment application, such as inability to apply extra funds to principal or pay off smaller loans more quickly.\\n- Errors or discrepancies in loan account status, balances, or reports, including being reported as delinquent without justification.\\n- Challenges related to loan management, including improper loan transfers, lack of communication, and disputes over interest and repayment amounts.\\n- Issues with loan forgiveness, cancellation, or discharge, especially when loans are mismanaged or aggressively pursued despite borrower difficulties or legal protections.\\n\\nIn summary, the most common issues seem to revolve around poor loan management, miscommunication, incorrect account information, and difficulties in properly managing repayment or understanding loan details.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "43zdcdUydtXh",
    "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, yes, there are complaints that did not get handled in a timely manner. Specifically, at least two complaints were marked as \"Not in time\" for response:\\n\\n- Complaint ID 12709087 (submitted to MOHELA), which was marked as \"Timely response?\": No.\\n- Complaint ID 12832400 (submitted to Maximus Federal Services, Inc. KY), which was marked as \"Timely response?\": Yes (so this one was handled timely).\\n\\nAdditionally, multiple complaints mention long delays or lack of response over extended periods (e.g., over 1 year) and ongoing unresolved issues, indicating some complaints were indeed not addressed in a timely manner.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "lpG6rlvvvKFq",
    "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"People failed to pay back their loans primarily due to a variety of issues highlighted in the complaints, including:\\n\\n1. **Difficulty Managing Interest and Payment Options:** Many borrowers were offered limited options such as forbearance or deferment, but these led to interest accumulating, making it harder to pay down the principal or catch up on payments. Lowering payments often resulted in interest negating progress, extending the repayment period and increasing total debt.\\n\\n2. **Unclear or Uncommunicated Loan Terms and Transfers:** Borrowers reported being inadequately informed about when payments would resume, changes in loan servicers (e.g., from Great Lakes to Nelnet or Navient) without proper notification, or being unaware of the precise status of their loans. This lack of communication led to missed payments or delinquency.\\n\\n3. **Lack of Assistance or Proper Payment Plans:** Several complaints indicated that borrowers could not get proper repayment plans, or their attempts to request income-driven repayment plans or to establish payment schedules were ignored or met with denial.\\n\\n4. **Issues with Loan Servicers and Data Management:** Problems such as incorrect account information, inability to apply payments correctly (e.g., funds only applied to interest, not principal), and mishandling of payments or account transfers caused confusion and unintentional delinquency.\\n\\n5. **Systemic Lack of Transparency and Information:** Many borrowers felt misled or inadequately informed about their loan conditions, interest rates, accruing debt, and their rights, which contributed to financial hardship and difficulty in making timely payments.\\n\\n6. **Economic Hardship and Stagnant Wages:** Several borrowers cited economic hardshipsâ€”including unemployment, low wages, or job lossâ€”as reasons incapable of keeping up with payments, especially when compounded by rising interest or unmanageable debt burdens.\\n\\nOverall, the combination of poor communication, limited repayment options, servicer mishandling, and personal financial hardships contributed to many borrowers' inability to pay back their loans.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not have information about what happened to complaint ID 13347464.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"What happned to complaint ID 13347464?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsbfQmbr1leg"
   },
   "source": [
    "Overall, this is not bad! Let's see if we can make it better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft1vt8HPR16w"
   },
   "source": [
    "## Task 5: Best-Matching 25 (BM25) Retriever\n",
    "\n",
    "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
    "\n",
    "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
    "\n",
    "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "qdF4wuj5R-cG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIjJlBQ8drKH"
   },
   "source": [
    "We'll construct the same chain - only changing the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "WR15EQG7SLuw"
   },
   "outputs": [],
   "source": [
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gi-yXCDdvJk"
   },
   "source": [
    "Let's look at the responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "oY9qzmm3SOrF",
    "outputId": "4d4f450f-5978-460f-f242-b32407868353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, including issues with fees, payment application, and inaccurate or bad information about the loans. Specific sub-issues include disagreements over fees, trouble with how payments are being handled (such as applying payments correctly), and receiving incorrect information about loan balances or terms. \\n\\nTherefore, the most common issue is related to the borrowerâ€™s difficulties in dealing with lenders or servicers, often involving issues like misapplied payments, incorrect information, or dissatisfaction with how the loans are managed.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "igfinyneSQkh",
    "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, all the complaints detailed in the context were responded to by the companies with a response described as \"Closed with explanation,\" and the responses are marked as timely. Therefore, it appears that no complaints went unhandled or unresolved in a timely manner.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "w0H7pV_USSMQ",
    "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans for various reasons, including difficulties with payment plans, issues with loan servicing companies, lack of communication or delays in response from lenders, and problems related to errors or mismanagement by loan servicers. Specifically, some borrowers experienced problems with their payment plans or are struggling to navigate complex forbearance options, while others faced issues of unacknowledged forbearance requests or technical errors that led to missed payments or negative impacts on their credit scores. Additionally, some complaints indicate that borrowers were unaware of changes in loan servicing or transfers to new companies, and that poor communication and administrative errors contributed to their inability to repay loans effectively.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The complaint with ID 13347464 was submitted on 05/05/25 regarding issues with federal student loan servicing. The consumer was disputing two loans totaling $10,000, claiming that the lender failed to provide proof that the loans match the Master Promissory Note and incorrectly advised the consumer to file an FTC Identity Theft Report, despite no identity theft occurring. The company, Maximus Federal Services, responded by closing the complaint with an explanation, implying they believed their actions were appropriate under the law.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"What happned to complaint ID 13347464?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvg5xHaUdxCl"
   },
   "source": [
    "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ Question #1:\n",
    "\n",
    "Give an example query where BM25 is better than embeddings and justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer #1:\n",
    "\n",
    "I asked Cursor what kinds of queries would do better with a keyword search type approach (eg. BM25) vs an embedding type approach (see PROMPTS.md) and what I understand from that is that short queries that refer to entites with exact matches in documents within the corpus will do better with BM25. For example \"What happened to complaint ID 13347464?\". For the BM25 retriever it found the record but the naive retriever did not.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-dcbFn2vpZF"
   },
   "source": [
    "## Task 6: Contextual Compression (Using Reranking)\n",
    "\n",
    "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
    "\n",
    "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
    "\n",
    "The basic idea here is this:\n",
    "\n",
    "- We retrieve lots of documents that are very likely related to our query vector\n",
    "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
    "\n",
    "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
    "\n",
    "All we need to do is the following:\n",
    "\n",
    "- Create a basic retriever\n",
    "- Create a compressor (reranker, in this case)\n",
    "\n",
    "That's it!\n",
    "\n",
    "Let's see it in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "psHvO2K1v_ZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TA9RB2x-j7P"
   },
   "source": [
    "Let's create our chain again, and see how this does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "1BXqmxvHwX6T"
   },
   "outputs": [],
   "source": [
    "contextual_compression_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V3iGpokswcBb",
    "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, the most common issue with loans appears to be problems related to dealing with lenders or servicers, such as receiving bad or incorrect information, errors in loan balances, misapplied payments, and issues with loan transfers and handling. Many complaints also involve lack of communication, incorrect account information, and disputes over loan details, which can lead to customer confusion and credit reporting issues.'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "7u_k0i4OweUd",
    "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, there are indications that some complaints experienced delays in being handled. For example, the complaint about the student loan account review, submitted over a year prior and still unresolved after nearly 18 months, suggests a significant delay. Additionally, the complaint about payments not being applied to the account also mentions a follow-up, implying ongoing unresolved issues.\\n\\nHowever, for the complaint regarding the issue with auto pay, the response states \"Timely response? Yes,\" indicating that this particular complaint was addressed in a timely manner.\\n\\nTherefore, yes, some complaints did not get handled in a timely manner, as evidenced by the complaint about unresolved student loan issues that remained open despite a long duration.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "zn1EqaGqweXN",
    "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans mainly due to a combination of factors including a lack of understanding of the repayment obligations, poor communication from lenders or servicers, and the ongoing accumulation of interest which made the debt grow even while making payments. Additional reasons included being unaware of the need to repay loans, receiving incorrect or incomplete information about their loan status, and facing difficulties in managing payments due to economic hardships, stagnant wages, or the effects of interest accumulating on their loans. Some borrowers also experienced issues related to their loans being transferred without proper Notification, further complicating their ability to stay informed and make timely payments.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEbT0g2S-mZ4"
   },
   "source": [
    "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqbghrBEQNn5"
   },
   "source": [
    "## Task 7: Multi-Query Retriever\n",
    "\n",
    "Typically in RAG we have a single query - the one provided by the user.\n",
    "\n",
    "What if we had....more than one query!\n",
    "\n",
    "In essence, a Multi-Query Retriever works by:\n",
    "\n",
    "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
    "2. Retrieving documents for each query.\n",
    "3. Using all unique retrieved documents as context\n",
    "\n",
    "So, how is it to set-up? Not bad! Let's see it down below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "pfM26ReXQjzU"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "1vRc129jQ5WW"
   },
   "outputs": [],
   "source": [
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "CGgNuOb3Q3M9",
    "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The most common issue with loans, based on the provided complaints, appears to be problems related to dealing with lenders or servicers, especially involving mishandling of payments, misapplication of funds, confusing or incorrect loan balances, miscommunication, and issues with loan account management such as errors in loan balances, interest calculations, and account transfers. Many complaints also highlight a lack of transparency, inadequate communication, and wrongful reporting that negatively impacts borrowers' credit and financial stability.\\n\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "aAlSthxrRDBC",
    "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, yes, some complaints indicate that they were not handled in a timely manner. Specifically, there are multiple instances where responses from companies were delayed beyond the expected timeframe:\\n\\n- On 04/02/25, a complaint submitted to Maximus Federal Services, Inc. was marked as \"Timely response?\": No.\\n- On 04/01/25, a complaint to MOHELA was also marked as \"Timely response?\": No.\\n- Similarly, a complaint sent on 03/25/25 to MOHELA was marked as \"Timely response?\": No.\\n- Another complaint to Maximus on 04/30/25 was marked \"Timely response?\": Yes, so handled promptly in this case.\\n\\nAdditionally, some complaints reference ongoing issues that have persisted for over a year or more without resolution, despite multiple follow-ups and escalations, suggesting that certain issues were not addressed in a timely manner.\\n\\nIn summary, yes, there are complaints indicating that some issues were not handled promptly, with delays ranging from several days to over a year, depending on the case.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Uv1mpCK8REs4",
    "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans primarily due to a combination of factors including:\\n\\n1. Lack of clear information about repayment options: Many borrowers were not adequately informed about income-driven repayment plans, loan forgiveness programs, or the long-term impact of forbearance and deferment options. This led to unmanageable interest accumulation and difficult repayment scenarios.\\n\\n2. Mismanagement and misinformation by loan servicers: Several complaints highlight instances where servicers used tactics such as forbearance steering, wrongful denial or misapplication of payments, and providing incorrect loan balance information. These practices often kept borrowers in prolonged debt cycles and prevented access to better repayment options.\\n\\n3. Accruing interest and rising balances: Forbearance and deferment, while temporarily relieving payment obligations, often resulted in interest continuing to accrue and compound, vastly increasing the total amount owed over time.\\n\\n4. Unforeseen financial hardships and personal circumstances: Borrowers experienced financial hardships, unemployment, medical issues, or life crises that made it difficult to keep up with payments.\\n\\n5. Systemic issues and lack of transparency: Several complaints point to systemic failures such as errors in credit reporting, unauthorized loan transfers, and inadequate communication from servicers, which caused confusion, errors in account status, and damaged credit histories.\\n\\nIn essence, many borrowers struggled to pay back their loans not due to irresponsibility but because of systemic mismanagement, misleading practices by servicers, lack of adequate information, and unforeseen hardships, all compounded by interest accumulation.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ Question #2:\n",
    "\n",
    "Explain how generating multiple reformulations of a user query can improve recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer #2\n",
    "\n",
    "I posed this question to Cursor (see PROMPTS.md) and my understanding is this: when we generate multiple reformulations of the user query, we increase vocabulary diversity, query intent variations in the query resulting in (possibly) better semantic coverage and hence possibly more relevant chunks will be retrieved. Also sometimes the user did not write the query in a well structured or incomplete form, getting the LLM to reformulate the query could help correct that - thus leading to more relevant chunks being retireved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDEawBf_d_3G"
   },
   "source": [
    "## Task 8: Parent Document Retriever\n",
    "\n",
    "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
    "\n",
    "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
    "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
    "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
    "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
    "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
    "\n",
    "Okay, maybe that was a few steps - but the basic idea is this:\n",
    "\n",
    "- Search for small documents\n",
    "- Return big documents\n",
    "\n",
    "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
    "\n",
    "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "qJ53JJuMd_ZH"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "parent_docs = loan_complaint_data\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOpXfVUH3gL3"
   },
   "source": [
    "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
    "\n",
    "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzFc-_9HlGQ-",
    "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
   },
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_g95FA3s6w"
   },
   "source": [
    "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "BpWVjPf4fLUp"
   },
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoYmSWfE32Zo"
   },
   "source": [
    "By default, this is empty as we haven't added any documents - let's add some now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "iQ2ZzfKigMZc"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever.add_documents(parent_docs, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI7Tip1335rE"
   },
   "source": [
    "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "Qq_adt2KlSqp"
   },
   "outputs": [],
   "source": [
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNolUVQb4Apt"
   },
   "source": [
    "Let's give it a whirl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "TXB5i89Zly5W",
    "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The most common issue with loans, based on the provided complaints, appears to be problems related to **incorrect or misleading information on credit reports, mismanagement by servicers, and disputes over account balances or interest rates**. Many complaints involve errors in loan balances, misapplied payments, wrongful denials of payment plans, and inaccuracies in reporting to credit bureaus. These issues often stem from systemic errors, miscommunication, or misconduct by loan servicers.\\n\\nPlease let me know if you'd like more specific information or details on particular types of loan issues.\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V5F1T-wNl3cg",
    "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, several complaints were identified as not being handled in a timely manner. Specifically, the complaints regarding the student loan service issues filed with MOHELA on 03/28/25 and 04/11/25 both received responses marked as \"No\" for being timely responded to. Additionally, the complaint with Nelnet filed on 04/27/25 was marked as \"Yes\" for timely response, indicating it was handled promptly. \\n\\nTherefore, yes, some complaints, including those filed with MOHELA, did not get handled in a timely manner.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ZqARszGzvGcG",
    "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans for various reasons, including experiencing severe financial hardship, mismanagement or misrepresentation by educational institutions, and issues related to loan servicing or communication failures. Specifically, some borrowers faced unexpected or undelayed payment obligations, lack of proper information about repayment terms or grace periods, and difficulties in managing or verifying their debts. Additionally, some individuals struggled due to joblessness or inability to secure employment in their field, which made consistent repayment impossible. Others encountered administrative problems such as improper reporting, failure to notify about payments, or issues stemming from the transfer or collection of their loans.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B41cj42s4DPM"
   },
   "source": [
    "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUrIBKl_TwS9"
   },
   "source": [
    "## Task 9: Ensemble Retriever\n",
    "\n",
    "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
    "\n",
    "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
    "\n",
    "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "8j7jpZsKTxic"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpo9Psl5hhJ-"
   },
   "source": [
    "We'll pack *all* of these retrievers together in an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "KZ__EZwpUKkd"
   },
   "outputs": [],
   "source": [
    "ensemble_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSsvHpRMj24L"
   },
   "source": [
    "Let's look at our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lMvqL88UQI-",
    "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, the most common issues with loans seem to involve:\\n\\n- Errors or bad information about loan balances and interest calculations.\\n- Difficulties with how payments are being handled, including being unable to apply additional payments to principal.\\n- Lack of transparency and inadequate communication about loan terms, balances, and changes.\\n- Unauthorized transfer or mishandling of loans.\\n- Incorrect or inconsistent reporting of account status and delinquency.\\n- Mishandling of loan consolidation, including lack of proper disclosure.\\n- Problems with repayment plans, interest accrual during forbearance, and changes in loan classifications.\\n- Disputes over loan ownership, validation, or fraudulent activity.\\n\\nWhile the specific most common issue isn't directly quantified in the data, a recurring theme is that many complaints involve **errors or bad information about loan balances, interest, and account status**, as well as poor handling of repayment and communication problems.\\n\\nTherefore, the most common issue appears to be:\\n\\n**Errors, mismanagement, or bad information regarding loan balances, interest, and account status, often compounded by poor communication and handling by loan servicers.**\\n\\nIf you need a precise single most common issue, I would have to say **errors in loan information and mishandling by servicers** seem to be the prevalent problem.\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "MNFWLYECURI1",
    "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, yes, some complaints indicate that complaints were not handled in a timely manner. Specifically:\\n\\n- Complaint ID 12744910 (row 66) from 04/18/25 about payments showing late; response was \"Yes\" to timely response but indicates delays of 10 days due to action not being taken within promised time frame.\\n- Complaint ID 12668396 (row 95) from 04/21/25 about account delinquencies and lengthy wait times; response was \"No\" to timely response, and the complaint notes over a month of trying to get assistance with unacceptable wait times.\\n- Complaint ID 12935889 (row 89) from 04/11/25 about account status and long wait times of over four hours; response was \"No\" to timely response.\\n- Several other complaints mention ongoing issues with delays, repeated follow-ups, or being left unaddressed over prolonged periods.\\n\\nIn summary, multiple complaints reflect that issues were not resolved in a timely manner, with delays ranging from several days to over a year, and some responses explicitly indicate failure to respond promptly.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "A7qbHfWgUR4c",
    "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans mainly due to a combination of mismanagement by loan servicers, lack of clear communication, and adverse economic circumstances. Many borrowers were not properly informed about the status of their loans, whether they were in deferment or forbearance, or the changes in loan transfer or servicing companies that affected their payment obligations. This lack of transparency often led to unintentional missed payments or late payments, negatively impacting their credit scores.\\n\\nAdditionally, various reports indicate that borrowers were steered into long-term forbearances instead of more manageable repayment plans like income-driven repayment or loan rehabilitation, which could have helped them avoid interest accumulation and default. Some borrowers also experienced issues such as incorrect or inconsistent loan account information, unauthorized transfer of loans, and failure to receive timely notices, which contributed to their inability to keep up with payments.\\n\\nEconomic hardships, including unemployment, medical issues, or unexpected life events, further hindered repayment efforts. The combination of systemic issues in loan management, poor communication from servicers, and financial difficulties caused many borrowers to struggle with consistently making payments or successfully managing their student debt repayment.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MopbkNJAXVaN"
   },
   "source": [
    "## Task 10: Semantic Chunking\n",
    "\n",
    "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
    "\n",
    "Essentially, Semantic Chunking is implemented by:\n",
    "\n",
    "1. Embedding all sentences in the corpus.\n",
    "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
    "  - `percentile`\n",
    "  - `standard_deviation`\n",
    "  - `interquartile`\n",
    "  - `gradient`\n",
    "3. Each sequence of related sentences is kept as a document!\n",
    "\n",
    "Let's see how to implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9ciZbFEldv_"
   },
   "source": [
    "We'll use the `percentile` thresholding method for this example which will:\n",
    "\n",
    "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "66EIEWiEYl5y"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqoKmz12mhRW"
   },
   "source": [
    "Now we can split our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "ROcV7o68ZIq7"
   },
   "outputs": [],
   "source": [
    "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8-LNC-Xmjex"
   },
   "source": [
    "Let's create a new vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "h3sl9QjyZhIe"
   },
   "outputs": [],
   "source": [
    "semantic_vectorstore = Qdrant.from_documents(\n",
    "    semantic_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh_r_-LHmmKn"
   },
   "source": [
    "We'll use naive retrieval for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "odVyDUHwZftc"
   },
   "outputs": [],
   "source": [
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkeiv_ojmp6G"
   },
   "source": [
    "Finally we can create our classic chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "xWE_0J0mZveG"
   },
   "outputs": [],
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5pfjLQ3ms9_"
   },
   "source": [
    "And view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lN2j-e4Z0SD",
    "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common issue with loans, based on the complaints provided, appears to be miscommunication and errors in the servicing and reporting of student loans. Specific recurring issues include:\\n\\n- Trouble with understanding and managing payment plans, such as incorrect billing amounts, failure to process applications, and issues with auto-debit setup.\\n- Problems related to the status of loans, including default notices, inaccurate reporting, and delays or discrepancies in account information.\\n- Lack of transparency and communication from loan servicers regarding account status, payment requirements, or changes in servicing.\\n\\nOverall, a significant number of complaints involve improper handling, reporting errors, or lack of clear communication from loan servicers, leading to borrower confusion, financial harm, and legal concerns.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "xdqfBH1SZ3f9",
    "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, yes, some complaints were not handled in a timely manner. Specifically, there are instances where complaints were marked as \"Closed with explanation\" and note that the company did not respond to the complaint or did so after the deadline. For example, complaints related to Nelnet, Inc. from IN and NJ did not receive prompt responses, and in some cases, the level of response or resolution suggests delays or lack of timely handling. \\n\\nHowever, the record also indicates that some complaints received responses marked as \"Yes\" for timely response, meaning they were handled within an acceptable timeframe.  \\n\\nIn summary, while many complaints were responded to in a timely manner, there are notable cases where handling was delayed or responses were lacking, indicating that some complaints did not get handled promptly.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "rAcAObZnZ4o6",
    "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans for various reasons, including difficulties with loan servicing, lack of transparency from lenders, disputes over the legitimacy or accuracy of reported debt, and issues related to the handling or verification of their loan information. Some faced problems with improper or delayed payments, miscommunications, or legal disputes. In certain cases, borrowers believed their loans had been improperly reported, were in default despite never being in default, or faced delays and errors in payment processing. Additionally, some borrowers experienced problems with loan forgiveness, discharge, or the legality of their debt due to legislative or administrative issues.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ Question #3:\n",
    "\n",
    "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer #3:\n",
    "\n",
    "I posed the question of what happens when we apply semantic chunking to documents with short and highly repetitive sentences to Cursor (see PROMPTS.md) and my understanding is this: since semantic chunking relies on semantic variation to detect natural breakpoints and short, repetitive sentences have high similarity and low variation it could lead to very large meaningless chunks or over-fragmentation (tiny+meaningless chunks). To adjust the algorithm to work better, one way would be to exploit structure (eg. TOC Chunker) since the document is already somewhat structured for context coherence in the sections:\n",
    "\n",
    "1. to prevent too large chunks from being created\n",
    "\n",
    "if the sentences cross a section then start a new chunk\n",
    "\n",
    "2. to prevent too small chunks from being created\n",
    "\n",
    "if too few sentences then continue until you hit a section boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk2n3-pnVWDJ"
   },
   "source": [
    "# ðŸ¤ Breakout Room Part #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SkJLYwMVZkj"
   },
   "source": [
    "#### ðŸ—ï¸ Activity #1\n",
    "\n",
    "Your task is to evaluate the various Retriever methods against eachother.\n",
    "\n",
    "You are expected to:\n",
    "\n",
    "1. Create a \"golden dataset\"\n",
    " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
    "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
    " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
    "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
    "\n",
    "Your analysis should factor in:\n",
    "  - Cost\n",
    "  - Latency\n",
    "  - Performance\n",
    "\n",
    "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWAr16a5XMub"
   },
   "source": [
    "##### HINTS:\n",
    "\n",
    "- LangSmith provides detailed information about latency and cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity #1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "tgDICngKXLGK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /Users/foohm/AIMakerSpace/AIE7.session9/09_Advanced_Retrieval_Eval/venv/bin/pip: /Users/foohm/AIMakerSpace/AIE7/09_Advanced_Retrieval_Eval/venv/bin/python3.13: bad interpreter: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(89217) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for evaluation\n",
    "!pip install -q ragas langsmith langchain-community datasets pandas numpy pillow rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API keys configured successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from uuid import uuid4\n",
    "\n",
    "# LangSmith API configuration\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key: \")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM AdvRetrievalRAG - {uuid4().hex[0:8]}\"\n",
    "\n",
    "# Confirm OpenAI API key is set (already configured in earlier cells)\n",
    "#if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "print(\"âœ… API keys configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Golden Data Set\n",
    "\n",
    "We'll create a synthetic evaluation dataset using RAGAS synthetic data generation from the loan complaint data, following the \"Abstracted SDG\" approach.\n",
    "\n",
    "This code has been extracted to a separate notebook `Golden_Dataset_Generator_Standalone.ipynb` so that we don't generate the golden data set each time we run the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"loan-complaints-golden-dataset-standalone-20250725-105810\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Sanity Checkpoint\n",
    "\n",
    "Using the golden dataset and the evaluators, we'll evaluate the `naive_retriever_chain` and extract overall metrics including average Cost and Latency from LangSmith API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Loading existing golden dataset for RAGAS evaluation...\n",
      "âœ… Loaded 15 examples from golden dataset: loan-complaints-golden-dataset-standalone-20250725-105810\n",
      "ðŸ“‹ Sample questions from golden dataset:\n",
      "  1. How does the illegal access by DOGE-related ventures, as described in the context, relate to the vio...\n",
      "  2. How does the CFPB's involvement relate to the failure of TransUnion to properly investigate and repo...\n",
      "  3. How do violations of the Higher Education Act and FERPA relate to the data breaches and mishandling ...\n",
      "\n",
      "ðŸ“Š Golden dataset ready for evaluation with 15 test cases\n"
     ]
    }
   ],
   "source": [
    "# Use Existing Golden Dataset for Direct RAGAS Evaluation\n",
    "from langsmith import Client\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"ðŸš€ Loading existing golden dataset for RAGAS evaluation...\")\n",
    "\n",
    "# Initialize LangSmith client to load the golden dataset\n",
    "client = Client()\n",
    "\n",
    "try:\n",
    "    # Get examples from the existing golden dataset\n",
    "    golden_examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "    print(f\"âœ… Loaded {len(golden_examples)} examples from golden dataset: {dataset_name}\")\n",
    "    \n",
    "    # Convert to format compatible with direct RAGAS evaluation\n",
    "    evaluation_data = []\n",
    "    for example in golden_examples:\n",
    "        eval_sample = {\n",
    "            \"user_input\": example.inputs[\"question\"],\n",
    "            \"reference_contexts\": example.metadata.get(\"reference_contexts\", []),\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            # These will be populated during evaluation:\n",
    "            \"response\": None,\n",
    "            \"retrieved_contexts\": None\n",
    "        }\n",
    "        evaluation_data.append(eval_sample)\n",
    "    \n",
    "    # Create a simple class to hold evaluation data (similar to RAGAS format)\n",
    "    class EvaluationSample:\n",
    "        def __init__(self, data):\n",
    "            self.user_input = data[\"user_input\"]\n",
    "            self.reference_contexts = data[\"reference_contexts\"]\n",
    "            self.reference = data[\"reference\"]\n",
    "            self.response = data[\"response\"]\n",
    "            self.retrieved_contexts = data[\"retrieved_contexts\"]\n",
    "    \n",
    "    # Convert to evaluation dataset format\n",
    "    evaluation_dataset = [EvaluationSample(data) for data in evaluation_data]\n",
    "    \n",
    "    print(\"ðŸ“‹ Sample questions from golden dataset:\")\n",
    "    for i, sample in enumerate(evaluation_dataset[:3]):\n",
    "        print(f\"  {i+1}. {sample.user_input[:100]}...\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Golden dataset ready for evaluation with {len(evaluation_dataset)} test cases\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading golden dataset: {e}\")\n",
    "    print(\"Please ensure the golden dataset has been created and is available in LangSmith\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the generic evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Direct RAGAS evaluation function created following your pandas + mean approach!\n"
     ]
    }
   ],
   "source": [
    "# Direct RAGAS Evaluation Function with Cost and Latency Tracking (CORRECTED APPROACH)\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from ragas import evaluate, EvaluationDataset, RunConfig\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall, Faithfulness, FactualCorrectness,\n",
    "    ResponseRelevancy, ContextRecall, NoiseSensitivity\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def evaluate_rag_chain_with_tracking(chain, dataset, method_name):\n",
    "    \"\"\"\n",
    "    Evaluate a RAG chain using correct RAGAS approach from semantic chunking notebook.\n",
    "    \n",
    "    Args:\n",
    "        chain: The RAG chain to evaluate\n",
    "        dataset: List of EvaluationSample objects with test cases\n",
    "        method_name: Name of the method for reporting\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with RAGAS metrics and cost/latency data\n",
    "    \"\"\"\n",
    "    print(f\"ðŸš€ Evaluating {method_name} retrieval chain...\")\n",
    "    print(f\"ðŸ“Š Test cases: {len(dataset)}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        # Step 1: Generate responses for each test case\n",
    "        print(\"   Step 1: Generating responses...\")\n",
    "        for i, test_sample in enumerate(dataset):\n",
    "            if i % 5 == 0:  # Progress indicator\n",
    "                print(f\"      Processing question {i+1}/{len(dataset)}\")\n",
    "            \n",
    "            # Get response from the RAG chain\n",
    "            response = chain.invoke({\"question\": test_sample.user_input})\n",
    "            \n",
    "            # Populate the sample with generated response and retrieved contexts\n",
    "            test_sample.response = response[\"response\"].content if hasattr(response[\"response\"], 'content') else str(response[\"response\"])\n",
    "            test_sample.retrieved_contexts = [\n",
    "                context.page_content for context in response[\"context\"]\n",
    "            ]\n",
    "        \n",
    "        # Step 2: Convert to pandas DataFrame with correct RAGAS format\n",
    "        print(\"   Step 2: Creating RAGAS DataFrame...\")\n",
    "        ragas_data = []\n",
    "        for sample in dataset:\n",
    "            ragas_data.append({\n",
    "                \"user_input\": sample.user_input,\n",
    "                \"response\": sample.response,\n",
    "                \"retrieved_contexts\": sample.retrieved_contexts,\n",
    "                \"reference_contexts\": sample.reference_contexts,\n",
    "                \"reference\": sample.reference\n",
    "            })\n",
    "        \n",
    "        ragas_df = pd.DataFrame(ragas_data)\n",
    "        \n",
    "        # Step 3: Create EvaluationDataset from pandas DataFrame\n",
    "        evaluation_dataset = EvaluationDataset.from_pandas(ragas_df)\n",
    "        \n",
    "        # Step 4: Set up evaluator LLM (following semantic chunking notebook approach)\n",
    "        evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0))\n",
    "        \n",
    "        # Step 5: Configure run settings\n",
    "        custom_run_config = RunConfig(timeout=360)\n",
    "        \n",
    "        # Step 6: Run RAGAS evaluation (following semantic chunking notebook approach)\n",
    "        print(\"   Step 3: Running RAGAS evaluation...\")\n",
    "        \n",
    "        ragas_results = evaluate(\n",
    "            dataset=evaluation_dataset,\n",
    "            metrics=[\n",
    "                LLMContextRecall(),\n",
    "                Faithfulness(), \n",
    "                FactualCorrectness(),\n",
    "                ResponseRelevancy(),\n",
    "                ContextRecall(),\n",
    "                NoiseSensitivity()\n",
    "            ],\n",
    "            llm=evaluator_llm,\n",
    "            run_config=custom_run_config\n",
    "        )\n",
    "        \n",
    "        # Step 7: Convert results to pandas and calculate means (following your approach)\n",
    "        print(\"   Step 4: Processing RAGAS results...\")\n",
    "        results_df = ragas_results.to_pandas()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Comprehensive results with cost and latency (following your pandas + mean approach)\n",
    "    results = {\n",
    "        # RAGAS metrics (calculated as means from pandas DataFrame)\n",
    "        \"context_entity_recall\": results_df['context_entity_recall'].mean() if 'context_entity_recall' in results_df.columns else 0.0,\n",
    "        \"faithfulness\": results_df['faithfulness'].mean() if 'faithfulness' in results_df.columns else 0.0,\n",
    "        \"factual_correctness\": results_df['factual_correctness(mode=f1)'].mean() if 'factual_correctness(mode=f1)' in results_df.columns else 0.0,\n",
    "        \"response_relevancy\": results_df['answer_relevancy'].mean() if 'answer_relevancy' in results_df.columns else 0.0,\n",
    "        \"context_recall\": results_df['context_recall'].mean() if 'context_recall' in results_df.columns else 0.0,\n",
    "        \"noise_sensitivity\": results_df['noise_sensitivity(mode=relevant)'].mean() if 'noise_sensitivity(mode=relevant)' in results_df.columns else 0.0,\n",
    "        \n",
    "        # Cost metrics (USD)\n",
    "        \"total_cost_usd\": cb.total_cost,\n",
    "        \"cost_per_query\": cb.total_cost / len(dataset) if len(dataset) > 0 else 0,\n",
    "        \n",
    "        # Token metrics\n",
    "        \"total_tokens\": cb.total_tokens,\n",
    "        \"prompt_tokens\": cb.prompt_tokens,\n",
    "        \"completion_tokens\": cb.completion_tokens,\n",
    "        \"tokens_per_query\": cb.total_tokens / len(dataset) if len(dataset) > 0 else 0,\n",
    "        \n",
    "        # Latency metrics (seconds)\n",
    "        \"total_latency_seconds\": end_time - start_time,\n",
    "        \"latency_per_query\": (end_time - start_time) / len(dataset) if len(dataset) > 0 else 0,\n",
    "        \n",
    "        # Efficiency metrics\n",
    "        \"cost_per_token\": cb.total_cost / cb.total_tokens if cb.total_tokens > 0 else 0,\n",
    "        \"tokens_per_second\": cb.total_tokens / (end_time - start_time) if (end_time - start_time) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… {method_name} evaluation completed!\")\n",
    "    print(f\"   ðŸ’° Total Cost: ${results['total_cost_usd']:.4f}\")\n",
    "    print(f\"   ðŸ”¢ Total Tokens: {results['total_tokens']:,}\")\n",
    "    print(f\"   â±ï¸ Total Time: {results['total_latency_seconds']:.2f} seconds\")\n",
    "    print(f\"   ðŸ“Š Average RAGAS Score: {sum([results[metric] for metric in ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']]) / 6:.4f}\")\n",
    "    \n",
    "    # Debug: Show available columns\n",
    "    print(f\"   ðŸ” Available RAGAS columns: {list(results_df.columns)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Direct RAGAS evaluation function created following your pandas + mean approach!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": [
     "naive_retrieval_evaluation"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATING NAIVE RETRIEVAL CHAIN\n",
      "================================================================================\n",
      "ðŸš€ Evaluating Naive retrieval chain...\n",
      "ðŸ“Š Test cases: 15\n",
      "   Step 1: Generating responses...\n",
      "      Processing question 1/15\n",
      "      Processing question 6/15\n",
      "      Processing question 11/15\n",
      "   Step 2: Creating RAGAS DataFrame...\n",
      "   Step 3: Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02c0861a76445dabc5fa164dfb7966c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[60]: OutputParserException(Invalid json output: Since the resumption of federal loan payments, I have received little to no contact. The last time I called, I was told I was in forbearance until 2040, but there is nothing in writing. I am having trouble logging in with my credentials, and when I call for assistance, there is a long wait followed by a disconnection once someone finally answers. The lack of transparency and accountability has caused undue stress.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step 4: Processing RAGAS results...\n",
      "âœ… Naive evaluation completed!\n",
      "   ðŸ’° Total Cost: $0.2096\n",
      "   ðŸ”¢ Total Tokens: 1,174,695\n",
      "   â±ï¸ Total Time: 420.01 seconds\n",
      "   ðŸ“Š Average RAGAS Score: 0.6360\n",
      "   ðŸ” Available RAGAS columns: ['user_input', 'retrieved_contexts', 'reference_contexts', 'response', 'reference', 'context_recall', 'faithfulness', 'factual_correctness(mode=f1)', 'answer_relevancy', 'noise_sensitivity(mode=relevant)']\n",
      "\n",
      "ðŸ“Š NAIVE RETRIEVAL RESULTS:\n",
      "----------------------------------------\n",
      "RAGAS Metrics:\n",
      "  Context Entity Recall: 0.0000\n",
      "  Faithfulness: 1.0000\n",
      "  Factual Correctness: 0.8487\n",
      "  Response Relevancy: 0.9284\n",
      "  Context Recall: 0.9667\n",
      "  Noise Sensitivity: 0.0722\n",
      "\n",
      "Cost & Performance:\n",
      "  Total Cost: $0.2096\n",
      "  Cost per Query: $0.0140\n",
      "  Total Tokens: 1,174,695\n",
      "  Tokens per Query: 78313.0\n",
      "  Total Latency: 420.01 seconds\n",
      "  Latency per Query: 28.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Naive Retrieval Chain\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING NAIVE RETRIEVAL CHAIN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy of the golden dataset for naive evaluation (each method needs its own copy)\n",
    "naive_dataset = [EvaluationSample({\n",
    "    \"user_input\": sample.user_input,\n",
    "    \"reference_contexts\": sample.reference_contexts,\n",
    "    \"reference\": sample.reference,\n",
    "    \"response\": None,\n",
    "    \"retrieved_contexts\": None\n",
    "}) for sample in evaluation_dataset]\n",
    "\n",
    "# Run evaluation with comprehensive tracking\n",
    "naive_results = evaluate_rag_chain_with_tracking(\n",
    "    chain=naive_retrieval_chain,\n",
    "    dataset=naive_dataset,\n",
    "    method_name=\"Naive\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š NAIVE RETRIEVAL RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"RAGAS Metrics:\")\n",
    "for metric in ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']:\n",
    "    print(f\"  {metric.replace('_', ' ').title()}: {naive_results[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nCost & Performance:\")\n",
    "print(f\"  Total Cost: ${naive_results['total_cost_usd']:.4f}\")\n",
    "print(f\"  Cost per Query: ${naive_results['cost_per_query']:.4f}\")\n",
    "print(f\"  Total Tokens: {naive_results['total_tokens']:,}\")\n",
    "print(f\"  Tokens per Query: {naive_results['tokens_per_query']:.1f}\")\n",
    "print(f\"  Total Latency: {naive_results['total_latency_seconds']:.2f} seconds\")\n",
    "print(f\"  Latency per Query: {naive_results['latency_per_query']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25 Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATING BM25 RETRIEVAL CHAIN\n",
      "================================================================================\n",
      "ðŸš€ Evaluating BM25 retrieval chain...\n",
      "ðŸ“Š Test cases: 15\n",
      "   Step 1: Generating responses...\n",
      "      Processing question 1/15\n",
      "      Processing question 6/15\n",
      "      Processing question 11/15\n",
      "   Step 2: Creating RAGAS DataFrame...\n",
      "   Step 3: Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257339fb01644f088b9915f36432fd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[23]: ValueError(setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step 4: Processing RAGAS results...\n",
      "âœ… BM25 evaluation completed!\n",
      "   ðŸ’° Total Cost: $0.1112\n",
      "   ðŸ”¢ Total Tokens: 652,058\n",
      "   â±ï¸ Total Time: 273.25 seconds\n",
      "   ðŸ“Š Average RAGAS Score: 0.6104\n",
      "   ðŸ” Available RAGAS columns: ['user_input', 'retrieved_contexts', 'reference_contexts', 'response', 'reference', 'context_recall', 'faithfulness', 'factual_correctness(mode=f1)', 'answer_relevancy', 'noise_sensitivity(mode=relevant)']\n",
      "\n",
      "ðŸ“Š BM25 RETRIEVAL RESULTS:\n",
      "----------------------------------------\n",
      "RAGAS Metrics:\n",
      "  Context Entity Recall: 0.0000\n",
      "  Faithfulness: 1.0000\n",
      "  Factual Correctness: 0.8327\n",
      "  Response Relevancy: 0.7972\n",
      "  Context Recall: 0.9667\n",
      "  Noise Sensitivity: 0.0661\n",
      "\n",
      "Cost & Performance:\n",
      "  Total Cost: $0.1112\n",
      "  Cost per Query: $0.0074\n",
      "  Total Tokens: 652,058\n",
      "  Tokens per Query: 43470.5\n",
      "  Total Latency: 273.25 seconds\n",
      "  Latency per Query: 18.22 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BM25 Retrieval Chain\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING BM25 RETRIEVAL CHAIN\") \n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy of the golden dataset for Multi-Query evaluation\n",
    "bm25_query_dataset = [EvaluationSample({\n",
    "    \"user_input\": sample.user_input,\n",
    "    \"reference_contexts\": sample.reference_contexts,\n",
    "    \"reference\": sample.reference,\n",
    "    \"response\": None,\n",
    "    \"retrieved_contexts\": None\n",
    "}) for sample in evaluation_dataset]\n",
    "\n",
    "# Run evaluation with comprehensive tracking\n",
    "bm25_results = evaluate_rag_chain_with_tracking(\n",
    "    chain=bm25_retrieval_chain,\n",
    "    dataset=bm25_query_dataset,\n",
    "    method_name=\"BM25\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š BM25 RETRIEVAL RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"RAGAS Metrics:\")\n",
    "for metric in ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']:\n",
    "    print(f\"  {metric.replace('_', ' ').title()}: {bm25_results[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nCost & Performance:\")\n",
    "print(f\"  Total Cost: ${bm25_results['total_cost_usd']:.4f}\")\n",
    "print(f\"  Cost per Query: ${bm25_results['cost_per_query']:.4f}\")\n",
    "print(f\"  Total Tokens: {bm25_results['total_tokens']:,}\")\n",
    "print(f\"  Tokens per Query: {bm25_results['tokens_per_query']:.1f}\")\n",
    "print(f\"  Total Latency: {bm25_results['total_latency_seconds']:.2f} seconds\")\n",
    "print(f\"  Latency per Query: {bm25_results['latency_per_query']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Query Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": [
     "bm25_retrieval_evaluation"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATING MULTI-QUERY RETRIEVAL CHAIN\n",
      "================================================================================\n",
      "ðŸš€ Evaluating Multi-Query retrieval chain...\n",
      "ðŸ“Š Test cases: 15\n",
      "   Step 1: Generating responses...\n",
      "      Processing question 1/15\n",
      "      Processing question 6/15\n",
      "      Processing question 11/15\n",
      "   Step 2: Creating RAGAS DataFrame...\n",
      "   Step 3: Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafc5ded16f1448894182ad2b776fc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[58]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-nano in organization org-JdudSPvfr6a6LEV8KvMepUOB on tokens per min (TPM): Limit 200000, Used 200000, Requested 6412. Please try again in 1.923s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[60]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-nano in organization org-JdudSPvfr6a6LEV8KvMepUOB on tokens per min (TPM): Limit 200000, Used 193711, Requested 8059. Please try again in 531ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[55]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-nano in organization org-JdudSPvfr6a6LEV8KvMepUOB on tokens per min (TPM): Limit 200000, Used 199221, Requested 6701. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[64]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-nano in organization org-JdudSPvfr6a6LEV8KvMepUOB on tokens per min (TPM): Limit 200000, Used 199908, Requested 8059. Please try again in 2.39s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[61]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-nano in organization org-JdudSPvfr6a6LEV8KvMepUOB on tokens per min (TPM): Limit 200000, Used 199405, Requested 8331. Please try again in 2.32s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step 4: Processing RAGAS results...\n",
      "âœ… Multi-Query evaluation completed!\n",
      "   ðŸ’° Total Cost: $0.2814\n",
      "   ðŸ”¢ Total Tokens: 1,570,529\n",
      "   â±ï¸ Total Time: 642.54 seconds\n",
      "   ðŸ“Š Average RAGAS Score: 0.6286\n",
      "   ðŸ” Available RAGAS columns: ['user_input', 'retrieved_contexts', 'reference_contexts', 'response', 'reference', 'context_recall', 'faithfulness', 'factual_correctness(mode=f1)', 'answer_relevancy', 'noise_sensitivity(mode=relevant)']\n",
      "\n",
      "ðŸ“Š MULTI-QUERY RETRIEVAL RESULTS:\n",
      "----------------------------------------\n",
      "RAGAS Metrics:\n",
      "  Context Entity Recall: 0.0000\n",
      "  Faithfulness: 0.9872\n",
      "  Factual Correctness: 0.8287\n",
      "  Response Relevancy: 0.8626\n",
      "  Context Recall: 0.9615\n",
      "  Noise Sensitivity: 0.1316\n",
      "\n",
      "Cost & Performance:\n",
      "  Total Cost: $0.2814\n",
      "  Cost per Query: $0.0188\n",
      "  Total Tokens: 1,570,529\n",
      "  Tokens per Query: 104701.9\n",
      "  Total Latency: 642.54 seconds\n",
      "  Latency per Query: 42.84 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Multi-Query Retrieval Chain\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING MULTI-QUERY RETRIEVAL CHAIN\") \n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy of the golden dataset for Multi-Query evaluation\n",
    "multi_query_dataset = [EvaluationSample({\n",
    "    \"user_input\": sample.user_input,\n",
    "    \"reference_contexts\": sample.reference_contexts,\n",
    "    \"reference\": sample.reference,\n",
    "    \"response\": None,\n",
    "    \"retrieved_contexts\": None\n",
    "}) for sample in evaluation_dataset]\n",
    "\n",
    "# Run evaluation with comprehensive tracking\n",
    "multi_query_results = evaluate_rag_chain_with_tracking(\n",
    "    chain=multi_query_retrieval_chain,\n",
    "    dataset=multi_query_dataset,\n",
    "    method_name=\"Multi-Query\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š MULTI-QUERY RETRIEVAL RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"RAGAS Metrics:\")\n",
    "for metric in ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']:\n",
    "    print(f\"  {metric.replace('_', ' ').title()}: {multi_query_results[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nCost & Performance:\")\n",
    "print(f\"  Total Cost: ${multi_query_results['total_cost_usd']:.4f}\")\n",
    "print(f\"  Cost per Query: ${multi_query_results['cost_per_query']:.4f}\")\n",
    "print(f\"  Total Tokens: {multi_query_results['total_tokens']:,}\")\n",
    "print(f\"  Tokens per Query: {multi_query_results['tokens_per_query']:.1f}\")\n",
    "print(f\"  Total Latency: {multi_query_results['total_latency_seconds']:.2f} seconds\")\n",
    "print(f\"  Latency per Query: {multi_query_results['latency_per_query']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parent Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATING PARENT-DOCUMENT RETRIEVAL CHAIN\n",
      "================================================================================\n",
      "ðŸš€ Evaluating Parent-Document retrieval chain...\n",
      "ðŸ“Š Test cases: 15\n",
      "   Step 1: Generating responses...\n",
      "      Processing question 1/15\n",
      "      Processing question 6/15\n",
      "      Processing question 11/15\n",
      "   Step 2: Creating RAGAS DataFrame...\n",
      "   Step 3: Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d55148f6bc4ac8997008e4d09d551c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step 4: Processing RAGAS results...\n",
      "âœ… Parent-Document evaluation completed!\n",
      "   ðŸ’° Total Cost: $0.1089\n",
      "   ðŸ”¢ Total Tokens: 622,929\n",
      "   â±ï¸ Total Time: 213.86 seconds\n",
      "   ðŸ“Š Average RAGAS Score: 0.6204\n",
      "   ðŸ” Available RAGAS columns: ['user_input', 'retrieved_contexts', 'reference_contexts', 'response', 'reference', 'context_recall', 'faithfulness', 'factual_correctness(mode=f1)', 'answer_relevancy', 'noise_sensitivity(mode=relevant)']\n",
      "\n",
      "ðŸ“Š PARENT-DOCUMENT RETRIEVAL RESULTS:\n",
      "----------------------------------------\n",
      "RAGAS Metrics:\n",
      "  Context Entity Recall: 0.0000\n",
      "  Faithfulness: 0.9939\n",
      "  Factual Correctness: 0.8033\n",
      "  Response Relevancy: 0.8593\n",
      "  Context Recall: 0.9333\n",
      "  Noise Sensitivity: 0.1323\n",
      "\n",
      "Cost & Performance:\n",
      "  Total Cost: $0.1089\n",
      "  Cost per Query: $0.0073\n",
      "  Total Tokens: 622,929\n",
      "  Tokens per Query: 41528.6\n",
      "  Total Latency: 213.86 seconds\n",
      "  Latency per Query: 14.26 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Parent-Document Retrieval Chain\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING PARENT-DOCUMENT RETRIEVAL CHAIN\") \n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy of the golden dataset for Parent-Document evaluation\n",
    "parent_document_dataset = [EvaluationSample({\n",
    "    \"user_input\": sample.user_input,\n",
    "    \"reference_contexts\": sample.reference_contexts,\n",
    "    \"reference\": sample.reference,\n",
    "    \"response\": None,\n",
    "    \"retrieved_contexts\": None\n",
    "}) for sample in evaluation_dataset]\n",
    "\n",
    "# Run evaluation with comprehensive tracking\n",
    "parent_document_results = evaluate_rag_chain_with_tracking(\n",
    "    chain=parent_document_retrieval_chain,\n",
    "    dataset=parent_document_dataset,\n",
    "    method_name=\"Parent-Document\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š PARENT-DOCUMENT RETRIEVAL RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"RAGAS Metrics:\")\n",
    "for metric in ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']:\n",
    "    print(f\"  {metric.replace('_', ' ').title()}: {parent_document_results[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nCost & Performance:\")\n",
    "print(f\"  Total Cost: ${parent_document_results['total_cost_usd']:.4f}\")\n",
    "print(f\"  Cost per Query: ${parent_document_results['cost_per_query']:.4f}\")\n",
    "print(f\"  Total Tokens: {parent_document_results['total_tokens']:,}\")\n",
    "print(f\"  Tokens per Query: {parent_document_results['tokens_per_query']:.1f}\")\n",
    "print(f\"  Total Latency: {parent_document_results['total_latency_seconds']:.2f} seconds\")\n",
    "print(f\"  Latency per Query: {parent_document_results['latency_per_query']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contextual Compression Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATING CONTEXTUAL COMPRESSION RETRIEVAL CHAIN\n",
      "================================================================================\n",
      "ðŸš€ Evaluating Contextual Compression retrieval chain...\n",
      "ðŸ“Š Test cases: 15\n",
      "   Step 1: Generating responses...\n",
      "      Processing question 1/15\n",
      "      Processing question 6/15\n",
      "      Processing question 11/15\n",
      "   Step 2: Creating RAGAS DataFrame...\n",
      "   Step 3: Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c43fccee02b4758a442e10a15efe1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step 4: Processing RAGAS results...\n",
      "âœ… Contextual Compression evaluation completed!\n",
      "   ðŸ’° Total Cost: $0.0959\n",
      "   ðŸ”¢ Total Tokens: 537,380\n",
      "   â±ï¸ Total Time: 168.95 seconds\n",
      "   ðŸ“Š Average RAGAS Score: 0.6158\n",
      "   ðŸ” Available RAGAS columns: ['user_input', 'retrieved_contexts', 'reference_contexts', 'response', 'reference', 'context_recall', 'faithfulness', 'factual_correctness(mode=f1)', 'answer_relevancy', 'noise_sensitivity(mode=relevant)']\n",
      "\n",
      "ðŸ“Š CONTEXTUAL COMPRESSION RETRIEVAL RESULTS:\n",
      "----------------------------------------\n",
      "RAGAS Metrics:\n",
      "  Context Entity Recall: 0.0000\n",
      "  Faithfulness: 0.9386\n",
      "  Factual Correctness: 0.8253\n",
      "  Response Relevancy: 0.9137\n",
      "  Context Recall: 0.9000\n",
      "  Noise Sensitivity: 0.1173\n",
      "\n",
      "Cost & Performance:\n",
      "  Total Cost: $0.0959\n",
      "  Cost per Query: $0.0064\n",
      "  Total Tokens: 537,380\n",
      "  Tokens per Query: 35825.3\n",
      "  Total Latency: 168.95 seconds\n",
      "  Latency per Query: 11.26 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Contextual Compression Retrieval Chain\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING CONTEXTUAL COMPRESSION RETRIEVAL CHAIN\") \n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy of the golden dataset for Contextual Compression evaluation\n",
    "contextual_compression_dataset = [EvaluationSample({\n",
    "    \"user_input\": sample.user_input,\n",
    "    \"reference_contexts\": sample.reference_contexts,\n",
    "    \"reference\": sample.reference,\n",
    "    \"response\": None,\n",
    "    \"retrieved_contexts\": None\n",
    "}) for sample in evaluation_dataset]\n",
    "\n",
    "# Run evaluation with comprehensive tracking\n",
    "contextual_compression_results = evaluate_rag_chain_with_tracking(\n",
    "    chain=contextual_compression_retrieval_chain,\n",
    "    dataset=contextual_compression_dataset,\n",
    "    method_name=\"Contextual Compression\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š CONTEXTUAL COMPRESSION RETRIEVAL RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"RAGAS Metrics:\")\n",
    "for metric in ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']:\n",
    "    print(f\"  {metric.replace('_', ' ').title()}: {contextual_compression_results[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nCost & Performance:\")\n",
    "print(f\"  Total Cost: ${contextual_compression_results['total_cost_usd']:.4f}\")\n",
    "print(f\"  Cost per Query: ${contextual_compression_results['cost_per_query']:.4f}\")\n",
    "print(f\"  Total Tokens: {contextual_compression_results['total_tokens']:,}\")\n",
    "print(f\"  Tokens per Query: {contextual_compression_results['tokens_per_query']:.1f}\")\n",
    "print(f\"  Total Latency: {contextual_compression_results['total_latency_seconds']:.2f} seconds\")\n",
    "print(f\"  Latency per Query: {contextual_compression_results['latency_per_query']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATING ENSEMBLE RETRIEVAL CHAIN\n",
      "================================================================================\n",
      "ðŸš€ Evaluating Ensemble retrieval chain...\n",
      "ðŸ“Š Test cases: 15\n",
      "   Step 1: Generating responses...\n",
      "      Processing question 1/15\n",
      "      Processing question 6/15\n",
      "      Processing question 11/15\n",
      "   Step 2: Creating RAGAS DataFrame...\n",
      "   Step 3: Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f3544a1be843bf8ddc61e413c1be76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[49]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-nano in organization org-JdudSPvfr6a6LEV8KvMepUOB on tokens per min (TPM): Limit 200000, Used 197898, Requested 11946. Please try again in 2.953s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[70]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1-nano in organization org-JdudSPvfr6a6LEV8KvMepUOB on tokens per min (TPM): Limit 200000, Used 199331, Requested 6430. Please try again in 1.728s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step 4: Processing RAGAS results...\n",
      "âœ… Ensemble evaluation completed!\n",
      "   ðŸ’° Total Cost: $0.3221\n",
      "   ðŸ”¢ Total Tokens: 1,863,670\n",
      "   â±ï¸ Total Time: 670.08 seconds\n",
      "   ðŸ“Š Average RAGAS Score: 0.6259\n",
      "   ðŸ” Available RAGAS columns: ['user_input', 'retrieved_contexts', 'reference_contexts', 'response', 'reference', 'context_recall', 'faithfulness', 'factual_correctness(mode=f1)', 'answer_relevancy', 'noise_sensitivity(mode=relevant)']\n",
      "\n",
      "ðŸ“Š ENSEMBLE RETRIEVAL RESULTS:\n",
      "----------------------------------------\n",
      "RAGAS Metrics:\n",
      "  Context Entity Recall: 0.0000\n",
      "  Faithfulness: 0.9973\n",
      "  Factual Correctness: 0.8333\n",
      "  Response Relevancy: 0.7936\n",
      "  Context Recall: 1.0000\n",
      "  Noise Sensitivity: 0.1315\n",
      "\n",
      "Cost & Performance:\n",
      "  Total Cost: $0.3221\n",
      "  Cost per Query: $0.0215\n",
      "  Total Tokens: 1,863,670\n",
      "  Tokens per Query: 124244.7\n",
      "  Total Latency: 670.08 seconds\n",
      "  Latency per Query: 44.67 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Ensemble Retrieval Chain\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING ENSEMBLE RETRIEVAL CHAIN\") \n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy of the golden dataset for Ensemble evaluation\n",
    "ensemble_dataset = [EvaluationSample({\n",
    "    \"user_input\": sample.user_input,\n",
    "    \"reference_contexts\": sample.reference_contexts,\n",
    "    \"reference\": sample.reference,\n",
    "    \"response\": None,\n",
    "    \"retrieved_contexts\": None\n",
    "}) for sample in evaluation_dataset]\n",
    "\n",
    "# Run evaluation with comprehensive tracking\n",
    "ensemble_results = evaluate_rag_chain_with_tracking(\n",
    "    chain=ensemble_retrieval_chain,\n",
    "    dataset=ensemble_dataset,\n",
    "    method_name=\"Ensemble\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š ENSEMBLE RETRIEVAL RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"RAGAS Metrics:\")\n",
    "for metric in ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']:\n",
    "    print(f\"  {metric.replace('_', ' ').title()}: {ensemble_results[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nCost & Performance:\")\n",
    "print(f\"  Total Cost: ${ensemble_results['total_cost_usd']:.4f}\")\n",
    "print(f\"  Cost per Query: ${ensemble_results['cost_per_query']:.4f}\")\n",
    "print(f\"  Total Tokens: {ensemble_results['total_tokens']:,}\")\n",
    "print(f\"  Tokens per Query: {ensemble_results['tokens_per_query']:.1f}\")\n",
    "print(f\"  Total Latency: {ensemble_results['total_latency_seconds']:.2f} seconds\")\n",
    "print(f\"  Latency per Query: {ensemble_results['latency_per_query']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATING SEMANTIC CHUNKING RETRIEVAL CHAIN\n",
      "================================================================================\n",
      "ðŸš€ Evaluating Semantic Chunking retrieval chain...\n",
      "ðŸ“Š Test cases: 15\n",
      "   Step 1: Generating responses...\n",
      "      Processing question 1/15\n",
      "      Processing question 6/15\n",
      "      Processing question 11/15\n",
      "   Step 2: Creating RAGAS DataFrame...\n",
      "   Step 3: Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52362f8b12c4346a931c7f0588335bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[59]: ValueError(setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.)\n",
      "Exception raised in Job[41]: ValueError(setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step 4: Processing RAGAS results...\n",
      "âœ… Semantic Chunking evaluation completed!\n",
      "   ðŸ’° Total Cost: $0.2005\n",
      "   ðŸ”¢ Total Tokens: 1,024,934\n",
      "   â±ï¸ Total Time: 438.78 seconds\n",
      "   ðŸ“Š Average RAGAS Score: 0.6398\n",
      "   ðŸ” Available RAGAS columns: ['user_input', 'retrieved_contexts', 'reference_contexts', 'response', 'reference', 'context_recall', 'faithfulness', 'factual_correctness(mode=f1)', 'answer_relevancy', 'noise_sensitivity(mode=relevant)']\n",
      "\n",
      "ðŸ“Š SEMANTIC CHUNKING RETRIEVAL RESULTS:\n",
      "----------------------------------------\n",
      "RAGAS Metrics:\n",
      "  Context Entity Recall: 0.0000\n",
      "  Faithfulness: 0.9846\n",
      "  Factual Correctness: 0.8473\n",
      "  Response Relevancy: 0.8683\n",
      "  Context Recall: 0.9667\n",
      "  Noise Sensitivity: 0.1718\n",
      "\n",
      "Cost & Performance:\n",
      "  Total Cost: $0.2005\n",
      "  Cost per Query: $0.0134\n",
      "  Total Tokens: 1,024,934\n",
      "  Tokens per Query: 68328.9\n",
      "  Total Latency: 438.78 seconds\n",
      "  Latency per Query: 29.25 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Semantic Chunking Retrieval Chain\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATING SEMANTIC CHUNKING RETRIEVAL CHAIN\") \n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a copy of the golden dataset for Semantic Chunking evaluation\n",
    "semantic_dataset = [EvaluationSample({\n",
    "    \"user_input\": sample.user_input,\n",
    "    \"reference_contexts\": sample.reference_contexts,\n",
    "    \"reference\": sample.reference,\n",
    "    \"response\": None,\n",
    "    \"retrieved_contexts\": None\n",
    "}) for sample in evaluation_dataset]\n",
    "\n",
    "# Run evaluation with comprehensive tracking\n",
    "semantic_results = evaluate_rag_chain_with_tracking(\n",
    "    chain=semantic_retrieval_chain,\n",
    "    dataset=semantic_dataset,\n",
    "    method_name=\"Semantic Chunking\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š SEMANTIC CHUNKING RETRIEVAL RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"RAGAS Metrics:\")\n",
    "for metric in ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']:\n",
    "    print(f\"  {metric.replace('_', ' ').title()}: {semantic_results[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nCost & Performance:\")\n",
    "print(f\"  Total Cost: ${semantic_results['total_cost_usd']:.4f}\")\n",
    "print(f\"  Cost per Query: ${semantic_results['cost_per_query']:.4f}\")\n",
    "print(f\"  Total Tokens: {semantic_results['total_tokens']:,}\")\n",
    "print(f\"  Tokens per Query: {semantic_results['tokens_per_query']:.1f}\")\n",
    "print(f\"  Total Latency: {semantic_results['total_latency_seconds']:.2f} seconds\")\n",
    "print(f\"  Latency per Query: {semantic_results['latency_per_query']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "COMPREHENSIVE COMPARISON: ALL 7 RAG RETRIEVAL APPROACHES\n",
      "====================================================================================================\n",
      "ðŸ“Š COMPREHENSIVE EVALUATION RESULTS:\n",
      "====================================================================================================\n",
      "          RAG Approach Context Entity Recall Faithfulness Factual Correctness Response Relevancy Context Recall Noise Sensitivity Average RAGAS Score Total Cost (USD) Cost per Query (USD) Total Tokens Tokens per Query Total Latency (seconds) Latency per Query (seconds) Cost Efficiency (Score/USD) Speed Efficiency (Score/second)\n",
      "                 Naive                0.0000       1.0000              0.8487             0.9284         0.9667            0.0722              0.6360          $0.2096              $0.0140    1,174,695          78313.0                  420.01                       28.00                        3.03                          0.0015\n",
      "                  BM25                0.0000       1.0000              0.8327             0.7972         0.9667            0.0661              0.6104          $0.1112              $0.0074      652,058          43470.5                  273.25                       18.22                        5.49                          0.0022\n",
      "           Multi-Query                0.0000       0.9872              0.8287             0.8626         0.9615            0.1316              0.6286          $0.2814              $0.0188    1,570,529         104701.9                  642.54                       42.84                        2.23                          0.0010\n",
      "       Parent-Document                0.0000       0.9939              0.8033             0.8593         0.9333            0.1323              0.6204          $0.1089              $0.0073      622,929          41528.6                  213.86                       14.26                        5.70                          0.0029\n",
      "Contextual Compression                0.0000       0.9386              0.8253             0.9137         0.9000            0.1173              0.6158          $0.0959              $0.0064      537,380          35825.3                  168.95                       11.26                        6.42                          0.0036\n",
      "              Ensemble                0.0000       0.9973              0.8333             0.7936         1.0000            0.1315              0.6259          $0.3221              $0.0215    1,863,670         124244.7                  670.08                       44.67                        1.94                          0.0009\n",
      "     Semantic Chunking                0.0000       0.9846              0.8473             0.8683         0.9667            0.1718              0.6398          $0.2005              $0.0134    1,024,934          68328.9                  438.78                       29.25                        3.19                          0.0015\n",
      "\n",
      "ðŸ† PERFORMANCE ANALYSIS:\n",
      "==================================================\n",
      "ðŸ¥‡ BEST OVERALL PERFORMANCE: Semantic Chunking\n",
      "   Average RAGAS Score: 0.6398\n",
      "\n",
      "ðŸ’° MOST COST-EFFECTIVE: Contextual Compression\n",
      "   Cost Efficiency: 6.42 score per USD\n",
      "\n",
      "âš¡ FASTEST APPROACH: Contextual Compression\n",
      "   Total Latency: 168.95 seconds\n",
      "\n",
      "ðŸŽ¯ INDIVIDUAL METRIC LEADERS:\n",
      "   Context Entity Recall: Naive (0.0000)\n",
      "   Faithfulness: Naive (1.0000)\n",
      "   Factual Correctness: Naive (0.8487)\n",
      "   Response Relevancy: Naive (0.9284)\n",
      "   Context Recall: Ensemble (1.0000)\n",
      "   Noise Sensitivity: Semantic Chunking (0.1718)\n",
      "\n",
      "====================================================================================================\n",
      "ðŸŽ‰ EVALUATION COMPLETE: All 7 RAG approaches have been comprehensively evaluated!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Comparison: All 7 RAG Approaches\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COMPREHENSIVE COMPARISON: ALL 7 RAG RETRIEVAL APPROACHES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Collect all results (NOTE: You need to run all evaluation cells above first)\n",
    "all_results = {\n",
    "    \"Naive\": naive_results,\n",
    "    \"BM25\": bm25_results,\n",
    "    \"Multi-Query\": multi_query_results,\n",
    "    \"Parent-Document\": parent_document_results,\n",
    "    \"Contextual Compression\": contextual_compression_results,\n",
    "    \"Ensemble\": ensemble_results,\n",
    "    \"Semantic Chunking\": semantic_results\n",
    "}\n",
    "\n",
    "# Create comprehensive comparison data\n",
    "comparison_data = {\n",
    "    \"RAG Approach\": [],\n",
    "    \"Context Entity Recall\": [],\n",
    "    \"Faithfulness\": [],\n",
    "    \"Factual Correctness\": [],\n",
    "    \"Response Relevancy\": [],\n",
    "    \"Context Recall\": [],\n",
    "    \"Noise Sensitivity\": [],\n",
    "    \"Average RAGAS Score\": [],\n",
    "    \"Total Cost (USD)\": [],\n",
    "    \"Cost per Query (USD)\": [],\n",
    "    \"Total Tokens\": [],\n",
    "    \"Tokens per Query\": [],\n",
    "    \"Total Latency (seconds)\": [],\n",
    "    \"Latency per Query (seconds)\": [],\n",
    "    \"Cost Efficiency (Score/USD)\": [],\n",
    "    \"Speed Efficiency (Score/second)\": []\n",
    "}\n",
    "\n",
    "# Populate data for each approach\n",
    "for approach_name, results in all_results.items():\n",
    "    avg_ragas_score = sum([results[metric] for metric in ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']]) / 6\n",
    "    \n",
    "    comparison_data[\"RAG Approach\"].append(approach_name)\n",
    "    comparison_data[\"Context Entity Recall\"].append(f\"{results['context_entity_recall']:.4f}\")\n",
    "    comparison_data[\"Faithfulness\"].append(f\"{results['faithfulness']:.4f}\")\n",
    "    comparison_data[\"Factual Correctness\"].append(f\"{results['factual_correctness']:.4f}\")\n",
    "    comparison_data[\"Response Relevancy\"].append(f\"{results['response_relevancy']:.4f}\")\n",
    "    comparison_data[\"Context Recall\"].append(f\"{results['context_recall']:.4f}\")\n",
    "    comparison_data[\"Noise Sensitivity\"].append(f\"{results['noise_sensitivity']:.4f}\")\n",
    "    comparison_data[\"Average RAGAS Score\"].append(f\"{avg_ragas_score:.4f}\")\n",
    "    comparison_data[\"Total Cost (USD)\"].append(f\"${results['total_cost_usd']:.4f}\")\n",
    "    comparison_data[\"Cost per Query (USD)\"].append(f\"${results['cost_per_query']:.4f}\")\n",
    "    comparison_data[\"Total Tokens\"].append(f\"{results['total_tokens']:,}\")\n",
    "    comparison_data[\"Tokens per Query\"].append(f\"{results['tokens_per_query']:.1f}\")\n",
    "    comparison_data[\"Total Latency (seconds)\"].append(f\"{results['total_latency_seconds']:.2f}\")\n",
    "    comparison_data[\"Latency per Query (seconds)\"].append(f\"{results['latency_per_query']:.2f}\")\n",
    "    comparison_data[\"Cost Efficiency (Score/USD)\"].append(f\"{avg_ragas_score / results['total_cost_usd'] if results['total_cost_usd'] > 0 else 0:.2f}\")\n",
    "    comparison_data[\"Speed Efficiency (Score/second)\"].append(f\"{avg_ragas_score / results['total_latency_seconds'] if results['total_latency_seconds'] > 0 else 0:.4f}\")\n",
    "\n",
    "# Create comprehensive comparison DataFrame\n",
    "comprehensive_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"ðŸ“Š COMPREHENSIVE EVALUATION RESULTS:\")\n",
    "print(\"=\" * 100)\n",
    "print(comprehensive_df.to_string(index=False))\n",
    "\n",
    "# Performance analysis\n",
    "print(\"\\nðŸ† PERFORMANCE ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find best performing approach by average RAGAS score\n",
    "ragas_scores = [sum([all_results[approach][metric] for metric in ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']]) / 6 for approach in all_results.keys()]\n",
    "best_approach_idx = ragas_scores.index(max(ragas_scores))\n",
    "best_approach = list(all_results.keys())[best_approach_idx]\n",
    "best_score = max(ragas_scores)\n",
    "\n",
    "print(f\"ðŸ¥‡ BEST OVERALL PERFORMANCE: {best_approach}\")\n",
    "print(f\"   Average RAGAS Score: {best_score:.4f}\")\n",
    "\n",
    "# Find most cost-effective approach\n",
    "cost_effectiveness = [ragas_scores[i] / all_results[list(all_results.keys())[i]]['total_cost_usd'] if all_results[list(all_results.keys())[i]]['total_cost_usd'] > 0 else 0 for i in range(len(ragas_scores))]\n",
    "most_cost_effective_idx = cost_effectiveness.index(max(cost_effectiveness))\n",
    "most_cost_effective = list(all_results.keys())[most_cost_effective_idx]\n",
    "\n",
    "print(f\"\\nðŸ’° MOST COST-EFFECTIVE: {most_cost_effective}\")\n",
    "print(f\"   Cost Efficiency: {max(cost_effectiveness):.2f} score per USD\")\n",
    "\n",
    "# Find fastest approach\n",
    "latencies = [all_results[approach]['total_latency_seconds'] for approach in all_results.keys()]\n",
    "fastest_approach_idx = latencies.index(min(latencies))\n",
    "fastest_approach = list(all_results.keys())[fastest_approach_idx]\n",
    "\n",
    "print(f\"\\nâš¡ FASTEST APPROACH: {fastest_approach}\")\n",
    "print(f\"   Total Latency: {min(latencies):.2f} seconds\")\n",
    "\n",
    "# Individual metric winners\n",
    "print(f\"\\nðŸŽ¯ INDIVIDUAL METRIC LEADERS:\")\n",
    "metrics = ['context_entity_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'context_recall', 'noise_sensitivity']\n",
    "for metric in metrics:\n",
    "    metric_scores = [all_results[approach][metric] for approach in all_results.keys()]\n",
    "    best_metric_idx = metric_scores.index(max(metric_scores))\n",
    "    best_metric_approach = list(all_results.keys())[best_metric_idx]\n",
    "    print(f\"   {metric.replace('_', ' ').title()}: {best_metric_approach} ({max(metric_scores):.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ðŸŽ‰ EVALUATION COMPLETE: All 7 RAG approaches have been comprehensively evaluated!\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Google Sheet - https://docs.google.com/spreadsheets/d/1kXyngWvo80IcnFxfVWKmefz9HLa9gzRvztBHjG4YrYg/edit?gid=605947219#gid=605947219 for better formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "The results are really interesting. \n",
    "\n",
    "* Right out of the bat, **Context Entity Recall** was all 0. I was pretty perplexed by this so I checked with Cursor (see PROMPTS.md - Prompt 5). The key idea behind Context Entity Recall are of course entities. The reason why we are seeing that metric to be zero is that a lot of the entities in the data are **anonymized** as they may be PII. So a value of zero is **to be expected**\n",
    "* Next is **Response Relevancy** as it is ultimately what we are looking for in a RAG. Interestingly the Naive approach has the best score but not by much to the next best - Contextual Compression. This makes sense cos the data is smaller so over optimizing on the chunking or retrieval could be counter productive. But let's look at the cost/latency: the contextual compression improved the latency by **half**. Cost wise it's also half but we are not including the cost of using Cohere's model so that needs to be factored in. In this particular case Naive was ~ $0.20 for the whole run and Context Compression was ~ $0.15 but this could change if pricing changes. \n",
    "* Next is **Faithfulness** - both Naive and BM25 have a score of 1 and all the rest are close (0.98-0.99) except for *Contextual Compression* (0.93). My hunch why this was the case is that compared to the exercise we did previously using PDF documents, we are dealing with customer complaints and the data being smaller, it would not lend itself well to the extra work of going thru the compression. So I checked with Cursor on this and that hunch seems to be confirmed. According to Cursor (see Prompt 6) we are doing more harm than good because (a) limited context pool - reranking removes useful info (b) over-optimization filtering context that's actually needed. In essence, less context == less faithful answers\n",
    "* **Factual Correctness** - Most of the approaches are close. Parent Doc is the worst but not by far. That makes sense cos the data is so small it actually hurts the overall Factual Correctness score when we try to split it even more. \n",
    "* **Context Recall** - Ensemble had a perfect score but Context Compression had the lowest score at 0.90 and we've seen this before in the last exercise cos just because your retrieved chunks are semantically similar doesn't mean it is relevant to the question. \n",
    "* **Noise Sensitivity** - both Naive and BM25 perform worse than the rest which makes sense since (a) BM25 is keyword search hence we are not using semantic similarity so it works best if the query contains important keywords otherwise it could be taken for a ride (b) as for Naive approach - all the other methods are trying to get more semantically similar chunks (than the Naive approach) into the context hence it makes sense that the Naive approach is has a lower noise sensitivity "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
