{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuttlefish3 RAG Chunking & Retrieval Evaluation\n",
    "\n",
    "This notebook evaluates various RAG retrieval methods using JIRA issue data from the Cuttlefish3 project.\n",
    "\n",
    "**Retrieval Methods Evaluated:**\n",
    "- Naive Retrieval\n",
    "- Best-Matching 25 (BM25)\n",
    "- Multi-Query Retrieval\n",
    "- Parent-Document Retrieval\n",
    "- Contextual Compression (Rerank)\n",
    "- Ensemble Retrieval\n",
    "- Semantic Chunking\n",
    "\n",
    "**Data Sources:**\n",
    "- JIRA Issues: `JIRA_OPEN_DATA_LARGESET_DATESHIFTED.csv`\n",
    "- Golden Dataset: `cuttlefish-jira-golden-dataset-20250731-122634`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Dependencies & API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages using exact versions from requirements.txt\n",
    "!pip install -q jupyter>=1.1.1\n",
    "!pip install -q langchain-experimental>=0.3.4\n",
    "!pip install -q langchain>=0.3.19\n",
    "!pip install -q \"cohere>=5.12.0,<5.13.0\"\n",
    "!pip install -q langchain-cohere==0.4.4\n",
    "!pip install -q langchain-openai>=0.3.7\n",
    "!pip install -q qdrant-client>=1.13.2\n",
    "!pip install -q rank-bm25>=0.2.2\n",
    "!pip install -q langchain-qdrant>=0.2.0\n",
    "!pip install -q ragas langsmith datasets pandas numpy pillow rapidfuzz langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API keys configured successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from uuid import uuid4\n",
    "\n",
    "# API Keys\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
    "    \n",
    "if \"COHERE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key: \")\n",
    "\n",
    "# LangSmith Configuration\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "if \"LANGCHAIN_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key: \")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Cuttlefish3 RAG Evaluation - {uuid4().hex[0:8]}\"\n",
    "\n",
    "print(\"✅ API keys configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading: JIRA Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JIRA issue data...\n",
      "✅ Loaded 1000 JIRA issue documents\n",
      "Sample document: Title: MAX_VERSIONS not respected.\n",
      "\n",
      "Description: Below is a report from the list.  I confirmed playing in shell that indeed we have this problem.  Lets fix for 0.2.1.{code}Hello.I made some tests with...\n",
      "\n",
      "Top 5 projects in dataset:\n",
      "  HBASE: 1000 issues\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Set CSV field size limit for large JIRA descriptions\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "print(\"Loading JIRA issue data...\")\n",
    "jira_documents = []\n",
    "\n",
    "with open('./JIRA_OPEN_DATA_LARGESET_DATESHIFTED.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    \n",
    "    for i, row in enumerate(reader):\n",
    "        title = row.get('title', '').strip()\n",
    "        description = row.get('description', '').strip()\n",
    "        \n",
    "        # Skip empty entries\n",
    "        if not title and not description:\n",
    "            continue\n",
    "            \n",
    "        # Create combined content for better RAG performance\n",
    "        if title and description:\n",
    "            content = f\"Title: {title}\\n\\nDescription: {description}\"\n",
    "        elif title:\n",
    "            content = f\"Title: {title}\"\n",
    "        else:\n",
    "            content = f\"Description: {description}\"\n",
    "        \n",
    "        # Create document with JIRA metadata\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"key\": row.get('key', ''),\n",
    "                \"project\": row.get('project', ''),\n",
    "                \"project_name\": row.get('project_name', ''),\n",
    "                \"priority\": row.get('priority', ''),\n",
    "                \"type\": row.get('type', ''),\n",
    "                \"status\": row.get('status', ''),\n",
    "                \"created\": row.get('created', ''),\n",
    "                \"title\": title,\n",
    "                \"description_length\": len(description)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        jira_documents.append(doc)\n",
    "        \n",
    "        # Limit to first 1000 documents for manageable processing\n",
    "        if len(jira_documents) >= 1000:\n",
    "            break\n",
    "\n",
    "print(f\"✅ Loaded {len(jira_documents)} JIRA issue documents\")\n",
    "print(f\"Sample document: {jira_documents[0].page_content[:200]}...\")\n",
    "\n",
    "# Show project distribution\n",
    "import pandas as pd\n",
    "projects = [doc.metadata['project'] for doc in jira_documents]\n",
    "project_counts = pd.Series(projects).value_counts().head(5)\n",
    "print(f\"\\nTop 5 projects in dataset:\")\n",
    "for project, count in project_counts.items():\n",
    "    print(f\"  {project}: {count} issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Components: Embeddings, VectorStore & LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Base components configured successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Base vectorstore for JIRA issues\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    jira_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"JiraIssues\"\n",
    ")\n",
    "\n",
    "# Chat model\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "# RAG prompt template\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a technical support assistant specializing in software issue resolution.\n",
    "Use the JIRA issue context provided below to answer the question accurately.\n",
    "\n",
    "If you don't know the answer based on the context, say so clearly.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "print(\"✅ Base components configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Builder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chain builder function ready\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def build_rag_chain(retriever, chain_name=\"RAG\"):\n",
    "    \"\"\"Build a RAG chain with the given retriever.\"\"\"\n",
    "    chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    print(f\"✅ {chain_name} chain created\")\n",
    "    return chain\n",
    "\n",
    "print(\"✅ Chain builder function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Method 1: Naive Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Naive chain created\n",
      "Test result: Common issues with HBase, based on the provided context, include:\n",
      "\n",
      "1. **Stuck Regions During Closure**: HBase can become stuck when closing regions concurrently, which can lead to performance issues o...\n"
     ]
    }
   ],
   "source": [
    "# Naive retriever - simple cosine similarity\n",
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "naive_chain = build_rag_chain(naive_retriever, \"Naive\")\n",
    "\n",
    "# Test query\n",
    "test_result = naive_chain.invoke({\"question\": \"What are common issues with HBase?\"})\n",
    "print(f\"Test result: {test_result['response'].content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Method 2: BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BM25 chain created\n",
      "Test result: Common issues with HBase, as indicated by the provided JIRA issues, include:\n",
      "\n",
      "1. **IOExceptions during Table Creation**: There are failure cases in the CreateTable Handler, particularly IOExceptions t...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# BM25 retriever - keyword-based sparse retrieval\n",
    "bm25_retriever = BM25Retriever.from_documents(jira_documents, k=10)\n",
    "bm25_chain = build_rag_chain(bm25_retriever, \"BM25\")\n",
    "\n",
    "# Test query\n",
    "test_result = bm25_chain.invoke({\"question\": \"What are common issues with HBase?\"})\n",
    "print(f\"Test result: {test_result['response'].content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Method 3: Multi-Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Query chain created\n",
      "Test result: Common issues with HBase, as indicated by the provided JIRA issues, include:\n",
      "\n",
      "1. **Security Configuration**: There are concerns about enforcing secure Hadoop as a requirement for secure HBase, which c...\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Multi-query retriever - generates multiple queries for better coverage\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, \n",
    "    llm=chat_model\n",
    ")\n",
    "multi_query_chain = build_rag_chain(multi_query_retriever, \"Multi-Query\")\n",
    "\n",
    "# Test query\n",
    "test_result = multi_query_chain.invoke({\"question\": \"What are common issues with HBase?\"})\n",
    "print(f\"Test result: {test_result['response'].content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Method 4: Parent Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parent Document chain created\n",
      "Test result: Common issues with HBase, based on the provided context, include:\n",
      "\n",
      "1. **Stuck Regions During Closure**: There is a known issue where HBase can become stuck when closing regions concurrently, which can...\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "# Parent document retriever - search small chunks, return large documents\n",
    "parent_docs = jira_documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "\n",
    "# Create new vectorstore for parent-child relationship\n",
    "parent_client = QdrantClient(location=\":memory:\")\n",
    "parent_client.create_collection(\n",
    "    collection_name=\"jira_parent_docs\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"jira_parent_docs\", \n",
    "    embedding=embeddings, \n",
    "    client=parent_client\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=parent_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "\n",
    "parent_document_retriever.add_documents(parent_docs, ids=None)\n",
    "parent_chain = build_rag_chain(parent_document_retriever, \"Parent Document\")\n",
    "\n",
    "# Test query\n",
    "test_result = parent_chain.invoke({\"question\": \"What are common issues with HBase?\"})\n",
    "print(f\"Test result: {test_result['response'].content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Method 5: Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Contextual Compression chain created\n",
      "Test result: Common issues with HBase, as indicated by the provided JIRA issues, include:\n",
      "\n",
      "1. **Race Conditions**: There are race conditions in the HCM.getMaster method that can stall clients. This issue arises wh...\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "# Contextual compression with reranking\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=naive_retriever\n",
    ")\n",
    "compression_chain = build_rag_chain(compression_retriever, \"Contextual Compression\")\n",
    "\n",
    "# Test query\n",
    "test_result = compression_chain.invoke({\"question\": \"What are common issues with HBase?\"})\n",
    "print(f\"Test result: {test_result['response'].content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Method 6: Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ensemble chain created\n",
      "Test result: Common issues with HBase, as indicated by the provided JIRA issues, include:\n",
      "\n",
      "1. **Concurrency Problems**: HBase can experience deadlocks or stalls when closing regions concurrently, as noted in HBASE...\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# Ensemble retriever - combines multiple retrievers\n",
    "retriever_list = [\n",
    "    bm25_retriever, \n",
    "    naive_retriever, \n",
    "    parent_document_retriever, \n",
    "    compression_retriever, \n",
    "    multi_query_retriever\n",
    "]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, \n",
    "    weights=equal_weighting\n",
    ")\n",
    "ensemble_chain = build_rag_chain(ensemble_retriever, \"Ensemble\")\n",
    "\n",
    "# Test query\n",
    "test_result = ensemble_chain.invoke({\"question\": \"What are common issues with HBase?\"})\n",
    "print(f\"Test result: {test_result['response'].content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Method 7: Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Semantic Chunking chain created\n",
      "✅ Created 73 semantic chunks from 50 documents\n",
      "Test result: Common issues with HBase, as indicated by the provided JIRA issues, include:\n",
      "\n",
      "1. **Corrupt HFiles**: Corrupt HFiles can lead to resource leaks and Out of Memory (OOM) errors on region servers. This ca...\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# Semantic chunking - split based on semantic similarity\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "\n",
    "# Use subset for semantic chunking due to processing time\n",
    "semantic_documents = semantic_chunker.split_documents(jira_documents[:50])\n",
    "\n",
    "semantic_vectorstore = Qdrant.from_documents(\n",
    "    semantic_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"JiraSemanticChunks\"\n",
    ")\n",
    "\n",
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "semantic_chain = build_rag_chain(semantic_retriever, \"Semantic Chunking\")\n",
    "\n",
    "print(f\"✅ Created {len(semantic_documents)} semantic chunks from {len(jira_documents[:50])} documents\")\n",
    "\n",
    "# Test query\n",
    "test_result = semantic_chain.invoke({\"question\": \"What are common issues with HBase?\"})\n",
    "print(f\"Test result: {test_result['response'].content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Golden Dataset for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading golden dataset: cuttlefish-jira-golden-dataset-20250731-122634\n",
      "✅ Loaded 15 examples from golden dataset\n",
      "📋 Sample questions:\n",
      "  1. Why does the NullPointerException occur in MetaEditor when trying to restore a snapshot, and how doe...\n",
      "  2. What issues arise from the custom implementation of ReplicationSink and how do they relate to the me...\n",
      "  3. What are the performance implications of excessive readpoint checks in MemStoreScanner and how do th...\n",
      "\n",
      "✅ Golden dataset ready: 15 test cases\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# Load golden dataset from LangSmith\n",
    "dataset_name = \"cuttlefish-jira-golden-dataset-20250731-122634\"\n",
    "client = Client()\n",
    "\n",
    "print(f\"Loading golden dataset: {dataset_name}\")\n",
    "\n",
    "try:\n",
    "    golden_examples = list(client.list_examples(dataset_name=dataset_name))\n",
    "    print(f\"✅ Loaded {len(golden_examples)} examples from golden dataset\")\n",
    "    \n",
    "    # Convert to evaluation format\n",
    "    evaluation_data = []\n",
    "    for example in golden_examples:\n",
    "        eval_sample = {\n",
    "            \"user_input\": example.inputs[\"question\"],\n",
    "            \"reference_contexts\": example.metadata.get(\"reference_contexts\", []),\n",
    "            \"reference\": example.outputs[\"answer\"],\n",
    "            \"response\": None,\n",
    "            \"retrieved_contexts\": None\n",
    "        }\n",
    "        evaluation_data.append(eval_sample)\n",
    "    \n",
    "    # Simple evaluation sample class\n",
    "    class EvaluationSample:\n",
    "        def __init__(self, data):\n",
    "            self.user_input = data[\"user_input\"]\n",
    "            self.reference_contexts = data[\"reference_contexts\"]\n",
    "            self.reference = data[\"reference\"]\n",
    "            self.response = data[\"response\"]\n",
    "            self.retrieved_contexts = data[\"retrieved_contexts\"]\n",
    "    \n",
    "    evaluation_dataset = [EvaluationSample(data) for data in evaluation_data]\n",
    "    \n",
    "    print(f\"📋 Sample questions:\")\n",
    "    for i, sample in enumerate(evaluation_dataset[:3]):\n",
    "        print(f\"  {i+1}. {sample.user_input[:100]}...\")\n",
    "    \n",
    "    print(f\"\\n✅ Golden dataset ready: {len(evaluation_dataset)} test cases\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading golden dataset: {e}\")\n",
    "    print(\"Please ensure the golden dataset exists in LangSmith\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation function ready\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from ragas import evaluate, EvaluationDataset, RunConfig\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall, Faithfulness, FactualCorrectness,\n",
    "    ResponseRelevancy, ContextRecall, NoiseSensitivity\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def evaluate_rag_chain(chain, dataset, method_name):\n",
    "    \"\"\"Evaluate a RAG chain using RAGAS metrics with cost and latency tracking.\"\"\"\n",
    "    print(f\"🚀 Evaluating {method_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        # Generate responses\n",
    "        print(\"   Generating responses...\")\n",
    "        for i, sample in enumerate(dataset):\n",
    "            if i % 5 == 0:\n",
    "                print(f\"      Processing {i+1}/{len(dataset)}\")\n",
    "            \n",
    "            response = chain.invoke({\"question\": sample.user_input})\n",
    "            sample.response = response[\"response\"].content if hasattr(response[\"response\"], 'content') else str(response[\"response\"])\n",
    "            sample.retrieved_contexts = [\n",
    "                context.page_content for context in response[\"context\"]\n",
    "            ]\n",
    "        \n",
    "        # Convert to RAGAS dataset\n",
    "        print(\"   Running RAGAS evaluation...\")\n",
    "        ragas_data = []\n",
    "        for sample in dataset:\n",
    "            ragas_data.append({\n",
    "                \"user_input\": sample.user_input,\n",
    "                \"response\": sample.response,\n",
    "                \"retrieved_contexts\": sample.retrieved_contexts,\n",
    "                \"reference_contexts\": sample.reference_contexts,\n",
    "                \"reference\": sample.reference\n",
    "            })\n",
    "        \n",
    "        ragas_df = pd.DataFrame(ragas_data)\n",
    "        evaluation_dataset = EvaluationDataset.from_pandas(ragas_df)\n",
    "        \n",
    "        # Configure evaluator\n",
    "        evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "        run_config = RunConfig(timeout=360)\n",
    "        \n",
    "        # Run evaluation\n",
    "        ragas_results = evaluate(\n",
    "            dataset=evaluation_dataset,\n",
    "            metrics=[\n",
    "                LLMContextRecall(),\n",
    "                Faithfulness(), \n",
    "                FactualCorrectness(),\n",
    "                ResponseRelevancy(),\n",
    "                ContextRecall(),\n",
    "                NoiseSensitivity()\n",
    "            ],\n",
    "            llm=evaluator_llm,\n",
    "            run_config=run_config\n",
    "        )\n",
    "        \n",
    "        results_df = ragas_results.to_pandas()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        \"method\": method_name,\n",
    "        \"context_recall\": results_df['context_recall'].mean() if 'context_recall' in results_df.columns else 0.0,\n",
    "        \"faithfulness\": results_df['faithfulness'].mean() if 'faithfulness' in results_df.columns else 0.0,\n",
    "        \"factual_correctness\": results_df['factual_correctness(mode=f1)'].mean() if 'factual_correctness(mode=f1)' in results_df.columns else 0.0,\n",
    "        \"response_relevancy\": results_df['answer_relevancy'].mean() if 'answer_relevancy' in results_df.columns else 0.0,\n",
    "        \"noise_sensitivity\": results_df['noise_sensitivity(mode=relevant)'].mean() if 'noise_sensitivity(mode=relevant)' in results_df.columns else 0.0,\n",
    "        \"total_cost_usd\": cb.total_cost,\n",
    "        \"cost_per_query\": cb.total_cost / len(dataset) if len(dataset) > 0 else 0,\n",
    "        \"total_tokens\": cb.total_tokens,\n",
    "        \"tokens_per_query\": cb.total_tokens / len(dataset) if len(dataset) > 0 else 0,\n",
    "        \"total_latency_seconds\": end_time - start_time,\n",
    "        \"latency_per_query\": (end_time - start_time) / len(dataset) if len(dataset) > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    # Calculate average score\n",
    "    metrics = ['context_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'noise_sensitivity']\n",
    "    results['average_score'] = sum([results[metric] for metric in metrics]) / len(metrics)\n",
    "    \n",
    "    print(f\"✅ {method_name} completed!\")\n",
    "    print(f\"   Average Score: {results['average_score']:.4f}\")\n",
    "    print(f\"   Cost: ${results['total_cost_usd']:.4f}\")\n",
    "    print(f\"   Latency: {results['total_latency_seconds']:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✅ Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "run_evaluators"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive evaluation...\n",
      "Evaluating 7 retrieval methods\n",
      "Using 15 test cases\n",
      "\n",
      "============================================================\n",
      "🚀 Evaluating Naive...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d57b5ef0224d85b7aed93ea29dad94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Naive completed!\n",
      "   Average Score: 0.6641\n",
      "   Cost: $0.2772\n",
      "   Latency: 614.85s\n",
      "============================================================\n",
      "🚀 Evaluating BM25...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4179cfcaca524dfba5fe15f883776862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BM25 completed!\n",
      "   Average Score: 0.5220\n",
      "   Cost: $0.2242\n",
      "   Latency: 488.77s\n",
      "============================================================\n",
      "🚀 Evaluating Multi-Query...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dacfcb829840429c38560458d743e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[54]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Query completed!\n",
      "   Average Score: 0.6941\n",
      "   Cost: $0.3040\n",
      "   Latency: 770.45s\n",
      "============================================================\n",
      "🚀 Evaluating Parent Document...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a47a96b37d4339b8fa0e37964f7228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parent Document completed!\n",
      "   Average Score: 0.5747\n",
      "   Cost: $0.1311\n",
      "   Latency: 332.73s\n",
      "============================================================\n",
      "🚀 Evaluating Contextual Compression...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8dc424c9ff4f93bc7d21b96084c0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Contextual Compression completed!\n",
      "   Average Score: 0.6352\n",
      "   Cost: $0.1364\n",
      "   Latency: 329.91s\n",
      "============================================================\n",
      "🚀 Evaluating Ensemble...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bab728fa424b3e967ab775bbfbd89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ensemble completed!\n",
      "   Average Score: 0.7237\n",
      "   Cost: $0.3978\n",
      "   Latency: 727.74s\n",
      "============================================================\n",
      "🚀 Evaluating Semantic Chunking...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e058f157454fb4a127f0fda9d650ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[59]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Semantic Chunking completed!\n",
      "   Average Score: 0.7222\n",
      "   Cost: $0.2776\n",
      "   Latency: 497.23s\n",
      "\n",
      "✅ Evaluation completed for 7/7 methods\n"
     ]
    }
   ],
   "source": [
    "# Prepare evaluation chains and datasets\n",
    "chains_to_evaluate = {\n",
    "    \"Naive\": naive_chain,\n",
    "    \"BM25\": bm25_chain,\n",
    "    \"Multi-Query\": multi_query_chain,\n",
    "    \"Parent Document\": parent_chain,\n",
    "    \"Contextual Compression\": compression_chain,\n",
    "    \"Ensemble\": ensemble_chain,\n",
    "    \"Semantic Chunking\": semantic_chain\n",
    "}\n",
    "\n",
    "print(\"🚀 Starting comprehensive evaluation...\")\n",
    "print(f\"Evaluating {len(chains_to_evaluate)} retrieval methods\")\n",
    "print(f\"Using {len(evaluation_dataset)} test cases\\n\")\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# Run evaluations (Note: This will take significant time)\n",
    "for method_name, chain in chains_to_evaluate.items():\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create fresh dataset copy for each evaluation\n",
    "    method_dataset = [EvaluationSample({\n",
    "        \"user_input\": sample.user_input,\n",
    "        \"reference_contexts\": sample.reference_contexts,\n",
    "        \"reference\": sample.reference,\n",
    "        \"response\": None,\n",
    "        \"retrieved_contexts\": None\n",
    "    }) for sample in evaluation_dataset]\n",
    "    \n",
    "    try:\n",
    "        results = evaluate_rag_chain(chain, method_dataset, method_name)\n",
    "        all_results[method_name] = results\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error evaluating {method_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Evaluation completed for {len(all_results)}/{len(chains_to_evaluate)} methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting evaluation for remaining methods...\n",
      "Evaluating 7 retrieval methods\n",
      "Methods to evaluate: ['Naive', 'BM25', 'Multi-Query', 'Parent Document', 'Contextual Compression', 'Ensemble', 'Semantic Chunking']\n",
      "Using 15 test cases\n",
      "\n",
      "============================================================\n",
      "🚀 Evaluating Naive...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c205198352fe4c32aad6c8415464ffde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Naive completed!\n",
      "   Average Score: 0.6603\n",
      "   Cost: $0.2819\n",
      "   Latency: 544.21s\n",
      "💾 Saved results to cuttlefish3_rag_evaluation_results.json\n",
      "============================================================\n",
      "🚀 Evaluating BM25...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99424d49ad3b47ea9c7821425d808fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BM25 completed!\n",
      "   Average Score: 0.5306\n",
      "   Cost: $0.2239\n",
      "   Latency: 482.87s\n",
      "💾 Saved results to cuttlefish3_rag_evaluation_results.json\n",
      "============================================================\n",
      "🚀 Evaluating Multi-Query...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3151014cd41041909a795e23791028e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Query completed!\n",
      "   Average Score: 0.6863\n",
      "   Cost: $0.3060\n",
      "   Latency: 678.13s\n",
      "💾 Saved results to cuttlefish3_rag_evaluation_results.json\n",
      "============================================================\n",
      "🚀 Evaluating Parent Document...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561db57ba2f94514ad96d176cccf95c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parent Document completed!\n",
      "   Average Score: 0.5816\n",
      "   Cost: $0.1314\n",
      "   Latency: 348.02s\n",
      "💾 Saved results to cuttlefish3_rag_evaluation_results.json\n",
      "============================================================\n",
      "🚀 Evaluating Contextual Compression...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b6b06f6c064f439989da6fd9c85618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Contextual Compression completed!\n",
      "   Average Score: 0.6413\n",
      "   Cost: $0.1359\n",
      "   Latency: 313.61s\n",
      "💾 Saved results to cuttlefish3_rag_evaluation_results.json\n",
      "============================================================\n",
      "🚀 Evaluating Ensemble...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4759f3ad83e4491ba3a4d9b12be03de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[5]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "Exception raised in Job[65]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ensemble completed!\n",
      "   Average Score: 0.6818\n",
      "   Cost: $0.3487\n",
      "   Latency: 704.28s\n",
      "💾 Saved results to cuttlefish3_rag_evaluation_results.json\n",
      "============================================================\n",
      "🚀 Evaluating Semantic Chunking...\n",
      "   Generating responses...\n",
      "      Processing 1/15\n",
      "      Processing 6/15\n",
      "      Processing 11/15\n",
      "   Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a77e492be0d47048e9fbfad853075e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[53]: TimeoutError()\n",
      "Exception raised in Job[59]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Semantic Chunking completed!\n",
      "   Average Score: 0.7439\n",
      "   Cost: $0.2775\n",
      "   Latency: 500.40s\n",
      "💾 Saved results to cuttlefish3_rag_evaluation_results.json\n",
      "\n",
      "✅ Evaluation completed for 7/7 methods\n",
      "\n",
      "💾 Final results saved to cuttlefish3_rag_evaluation_results.json\n",
      "📊 Total methods evaluated: 7\n",
      "🔍 To reload results later, simply re-run this cell\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Results file path\n",
    "results_file = \"cuttlefish3_rag_evaluation_results.json\"\n",
    "\n",
    "# Try to load existing results first\n",
    "all_results = {}\n",
    "if os.path.exists(results_file):\n",
    "    print(f\"🔄 Loading existing results from {results_file}\")\n",
    "    try:\n",
    "        with open(results_file, 'r') as f:\n",
    "            saved_data = json.load(f)\n",
    "            all_results = saved_data.get(\"results\", {})\n",
    "            print(f\"✅ Loaded {len(all_results)} existing evaluation results\")\n",
    "            print(f\"   Previously evaluated methods: {list(all_results.keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error loading existing results: {e}\")\n",
    "        all_results = {}\n",
    "\n",
    "# Prepare evaluation chains and datasets\n",
    "chains_to_evaluate = {\n",
    "    \"Naive\": naive_chain,\n",
    "    \"BM25\": bm25_chain,\n",
    "    \"Multi-Query\": multi_query_chain,\n",
    "    \"Parent Document\": parent_chain,\n",
    "    \"Contextual Compression\": compression_chain,\n",
    "    \"Ensemble\": ensemble_chain,\n",
    "    \"Semantic Chunking\": semantic_chain\n",
    "}\n",
    "\n",
    "# Filter out already completed evaluations\n",
    "remaining_chains = {name: chain for name, chain in chains_to_evaluate.items() \n",
    "                   if name not in all_results}\n",
    "\n",
    "if remaining_chains:\n",
    "    print(f\"\\n🚀 Starting evaluation for remaining methods...\")\n",
    "    print(f\"Evaluating {len(remaining_chains)} retrieval methods\")\n",
    "    print(f\"Methods to evaluate: {list(remaining_chains.keys())}\")\n",
    "    print(f\"Using {len(evaluation_dataset)} test cases\\n\")\n",
    "    \n",
    "    # Run evaluations for remaining methods\n",
    "    for method_name, chain in remaining_chains.items():\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create fresh dataset copy for each evaluation\n",
    "        method_dataset = [EvaluationSample({\n",
    "            \"user_input\": sample.user_input,\n",
    "            \"reference_contexts\": sample.reference_contexts,\n",
    "            \"reference\": sample.reference,\n",
    "            \"response\": None,\n",
    "            \"retrieved_contexts\": None\n",
    "        }) for sample in evaluation_dataset]\n",
    "        \n",
    "        try:\n",
    "            results = evaluate_rag_chain(chain, method_dataset, method_name)\n",
    "            all_results[method_name] = results\n",
    "            \n",
    "            # Save results immediately after each evaluation\n",
    "            save_data = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"dataset_info\": {\n",
    "                    \"dataset_name\": \"cuttlefish-jira-golden-dataset-20250731-122634\",\n",
    "                    \"num_test_cases\": len(evaluation_dataset),\n",
    "                    \"jira_documents_count\": len(jira_documents)\n",
    "                },\n",
    "                \"results\": all_results\n",
    "            }\n",
    "            \n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(save_data, f, indent=2)\n",
    "            print(f\"💾 Saved results to {results_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error evaluating {method_name}: {e}\")\n",
    "            # Save partial results even on error\n",
    "            save_data = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"dataset_info\": {\n",
    "                    \"dataset_name\": \"cuttlefish-jira-golden-dataset-20250731-122634\",\n",
    "                    \"num_test_cases\": len(evaluation_dataset),\n",
    "                    \"jira_documents_count\": len(jira_documents)\n",
    "                },\n",
    "                \"results\": all_results,\n",
    "                \"last_error\": {\n",
    "                    \"method\": method_name,\n",
    "                    \"error\": str(e),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "            }\n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(save_data, f, indent=2)\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n✅ Evaluation completed for {len(all_results)}/{len(chains_to_evaluate)} methods\")\n",
    "else:\n",
    "    print(f\"\\n✅ All evaluations already completed!\")\n",
    "    print(f\"Found results for: {list(all_results.keys())}\")\n",
    "\n",
    "# Final save\n",
    "final_save_data = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"dataset_info\": {\n",
    "        \"dataset_name\": \"cuttlefish-jira-golden-dataset-20250731-122634\",\n",
    "        \"num_test_cases\": len(evaluation_dataset),\n",
    "        \"jira_documents_count\": len(jira_documents)\n",
    "    },\n",
    "    \"results\": all_results\n",
    "}\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(final_save_data, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Final results saved to {results_file}\")\n",
    "print(f\"📊 Total methods evaluated: {len(all_results)}\")\n",
    "print(f\"🔍 To reload results later, simply re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Results (Optional)\n",
    "\n",
    "Run this cell if you want to load previously saved evaluation results without re-running evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load existing results without running evaluations\n",
    "import json\n",
    "import os\n",
    "\n",
    "results_file = \"cuttlefish3_rag_evaluation_results.json\"\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    print(f\"📂 Loading results from {results_file}\")\n",
    "    with open(results_file, 'r') as f:\n",
    "        saved_data = json.load(f)\n",
    "        all_results = saved_data.get(\"results\", {})\n",
    "        \n",
    "    print(f\"✅ Loaded evaluation results for {len(all_results)} methods:\")\n",
    "    for method, results in all_results.items():\n",
    "        print(f\"   • {method}: Average Score = {results['average_score']:.4f}\")\n",
    "        \n",
    "    print(f\"\\n📊 Dataset info:\")\n",
    "    dataset_info = saved_data.get(\"dataset_info\", {})\n",
    "    print(f\"   • Dataset: {dataset_info.get('dataset_name', 'N/A')}\")\n",
    "    print(f\"   • Test cases: {dataset_info.get('num_test_cases', 'N/A')}\")\n",
    "    print(f\"   • JIRA documents: {dataset_info.get('jira_documents_count', 'N/A')}\")\n",
    "    print(f\"   • Last updated: {saved_data.get('timestamp', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"❌ No saved results found at {results_file}\")\n",
    "    print(\"Run the evaluation cell above to generate results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "import_pandas"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CUTTLEFISH3 JIRA RAG EVALUATION RESULTS\n",
      "================================================================================\n",
      "📊 COMPREHENSIVE RESULTS:\n",
      "                Method Avg Score Context Recall Faithfulness Factual Correctness Response Relevancy Noise Sensitivity Total Cost Cost/Query Total Time Time/Query Tokens/Query\n",
      "                 Naive    0.6603         0.8722       0.7977              0.6787             0.8234            0.1294    $0.2819    $0.0188     544.2s      36.3s        77353\n",
      "                  BM25    0.5306         0.6444       0.7844              0.4860             0.6351            0.1031    $0.2239    $0.0149     482.9s      32.2s        58071\n",
      "           Multi-Query    0.6863         0.9222       0.7993              0.7000             0.8890            0.1209    $0.3060    $0.0204     678.1s      45.2s        85142\n",
      "       Parent Document    0.5816         0.6733       0.7914              0.5840             0.6293            0.2300    $0.1314    $0.0088     348.0s      23.2s        35763\n",
      "Contextual Compression    0.6413         0.7778       0.7800              0.5787             0.8854            0.1846    $0.1359    $0.0091     313.6s      20.9s        37376\n",
      "              Ensemble    0.6818         0.9556       0.8720              0.6293             0.9519            0.0000    $0.3487    $0.0232     704.3s      47.0s        99209\n",
      "     Semantic Chunking    0.7439         0.9556       0.8250              0.7560             0.9538            0.2290    $0.2775    $0.0185     500.4s      33.4s        76145\n",
      "\n",
      "🏆 PERFORMANCE ANALYSIS:\n",
      "--------------------------------------------------\n",
      "🥇 Best Overall: Semantic Chunking (Score: 0.7439)\n",
      "💰 Most Cost-Effective: Parent Document ($0.0088/query)\n",
      "⚡ Fastest: Contextual Compression (313.6s total)\n",
      "\n",
      "🎯 Metric Leaders:\n",
      "   Context Recall: Ensemble (0.9556)\n",
      "   Faithfulness: Ensemble (0.8720)\n",
      "   Factual Correctness: Semantic Chunking (0.7560)\n",
      "   Response Relevancy: Semantic Chunking (0.9538)\n",
      "   Noise Sensitivity: Parent Document (0.2300)\n",
      "\n",
      "================================================================================\n",
      "🎉 CUTTLEFISH3 JIRA RAG EVALUATION COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CUTTLEFISH3 JIRA RAG EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not all_results:\n",
    "    print(\"❌ No evaluation results available. Please run the evaluation cells above.\")\n",
    "else:\n",
    "    # Create comprehensive comparison DataFrame\n",
    "    comparison_data = []\n",
    "    \n",
    "    for method_name, results in all_results.items():\n",
    "        comparison_data.append({\n",
    "            \"Method\": method_name,\n",
    "            \"Avg Score\": f\"{results['average_score']:.4f}\",\n",
    "            \"Context Recall\": f\"{results['context_recall']:.4f}\",\n",
    "            \"Faithfulness\": f\"{results['faithfulness']:.4f}\",\n",
    "            \"Factual Correctness\": f\"{results['factual_correctness']:.4f}\",\n",
    "            \"Response Relevancy\": f\"{results['response_relevancy']:.4f}\",\n",
    "            \"Noise Sensitivity\": f\"{results['noise_sensitivity']:.4f}\",\n",
    "            \"Total Cost\": f\"${results['total_cost_usd']:.4f}\",\n",
    "            \"Cost/Query\": f\"${results['cost_per_query']:.4f}\",\n",
    "            \"Total Time\": f\"{results['total_latency_seconds']:.1f}s\",\n",
    "            \"Time/Query\": f\"{results['latency_per_query']:.1f}s\",\n",
    "            \"Tokens/Query\": f\"{results['tokens_per_query']:.0f}\"\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"📊 COMPREHENSIVE RESULTS:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Performance analysis\n",
    "    print(f\"\\n🏆 PERFORMANCE ANALYSIS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Best overall performance\n",
    "    best_method = max(all_results.items(), key=lambda x: x[1]['average_score'])\n",
    "    print(f\"🥇 Best Overall: {best_method[0]} (Score: {best_method[1]['average_score']:.4f})\")\n",
    "    \n",
    "    # Most cost-effective\n",
    "    cost_effective = min([r for r in all_results.items() if r[1]['total_cost_usd'] > 0], \n",
    "                        key=lambda x: x[1]['cost_per_query'])\n",
    "    print(f\"💰 Most Cost-Effective: {cost_effective[0]} (${cost_effective[1]['cost_per_query']:.4f}/query)\")\n",
    "    \n",
    "    # Fastest method\n",
    "    fastest = min(all_results.items(), key=lambda x: x[1]['total_latency_seconds'])\n",
    "    print(f\"⚡ Fastest: {fastest[0]} ({fastest[1]['total_latency_seconds']:.1f}s total)\")\n",
    "    \n",
    "    # Individual metric leaders\n",
    "    metrics = ['context_recall', 'faithfulness', 'factual_correctness', 'response_relevancy', 'noise_sensitivity']\n",
    "    print(f\"\\n🎯 Metric Leaders:\")\n",
    "    for metric in metrics:\n",
    "        leader = max(all_results.items(), key=lambda x: x[1][metric])\n",
    "        print(f\"   {metric.replace('_', ' ').title()}: {leader[0]} ({leader[1][metric]:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"🎉 CUTTLEFISH3 JIRA RAG EVALUATION COMPLETE!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the Gsheet version of this table here: https://docs.google.com/spreadsheets/d/1raoUdbfGGrQIicetcCilAQuSlpLUG8IQJLtV5Tv3ztk/edit?usp=sharing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Recommendations\n",
    "\n",
    "### Chunking:\n",
    "\n",
    "SemanticChunking performs better than the default RecursiveCharacterTextSplit across the board. Although it may seem that the higher Noise Sensitivity is bad that is not the case (checked with Cursor in Prompt 11 of PROMPTS.md) - a higher noise sensitivity means Semantic Chunking is more sensitive to context changes which is beneficial for the kind of queries we expect from this app ie. technical support scenarios. \n",
    "\n",
    "### Retrieval \n",
    "\n",
    "Overall if you look at the scores for Response Relevancy, the Ensemble approach scores really well but at a huge cost (47s/request). The next best is Multi-Query but it's not much better at 45s/request. Next is Contextual Compression that has a similar score to Multi-Query for Response Relevancy but at only 21s/request. \n",
    "\n",
    "Given this, we will use the following retrieval mechanisms:\n",
    "\n",
    "1. If the user needs an answer quickly eg. during a production incident - use the Contextual Compression retrieval \n",
    "2. If the user is able to wait for ~ 1min use the Ensemble retrieval\n",
    "\n",
    "We have also seen during the past assessment that there are some types of queries where a keyword search is able to find information but semantic similarity just doesn't retrieve results. These are usually the ones where key entities are being searched for eg. ticket numbers. As we are dealing with tickets, for this project we'll also include BM25 as another retrieval option. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
