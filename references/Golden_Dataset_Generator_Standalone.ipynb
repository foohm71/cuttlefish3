{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Golden Dataset Generator - Standalone\n",
        "\n",
        "This notebook creates a synthetic golden dataset using RAGAS from loan complaint data and uploads it to LangSmith.\n",
        "\n",
        "**Features:**\n",
        "- Self-contained (no dependencies on other notebooks)\n",
        "- Generates 15 synthetic Q&A pairs\n",
        "- Uploads dataset to LangSmith for evaluation tracking\n",
        "- Uses \"Abstracted SDG\" approach from RAGAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /Users/foohm/AIMakerSpace/AIE7.session9/09_Advanced_Retrieval_Eval/venv/bin/pip: /Users/foohm/AIMakerSpace/AIE7/09_Advanced_Retrieval_Eval/venv/bin/python3.13: bad interpreter: No such file or directory\n",
            "/bin/bash: /Users/foohm/AIMakerSpace/AIE7.session9/09_Advanced_Retrieval_Eval/venv/bin/pip: /Users/foohm/AIMakerSpace/AIE7/09_Advanced_Retrieval_Eval/venv/bin/python3.13: bad interpreter: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q ragas langsmith langchain-community datasets pandas numpy pillow rapidfuzz\n",
        "!pip install -q langchain-openai langchain-core qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: API Keys Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ API keys configured successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "from uuid import uuid4\n",
        "\n",
        "# OpenAI API Key\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# LangSmith API configuration\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "if \"LANGCHAIN_API_KEY\" not in os.environ:\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API Key: \")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - S09 - Golden Dataset Generation - {uuid4().hex[0:8]}\"\n",
        "\n",
        "print(\"✅ API keys configured successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Loan Complaint Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded 825 loan complaint documents\n",
            "Example document preview: The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new...\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load loan complaint data from CSV\n",
        "loader = CSVLoader(\n",
        "    file_path=\"./data/complaints.csv\",\n",
        "    metadata_columns=[\n",
        "        \"Date received\", \n",
        "        \"Product\", \n",
        "        \"Sub-product\", \n",
        "        \"Issue\", \n",
        "        \"Sub-issue\", \n",
        "        \"Consumer complaint narrative\", \n",
        "        \"Company public response\", \n",
        "        \"Company\", \n",
        "        \"State\", \n",
        "        \"ZIP code\", \n",
        "        \"Tags\", \n",
        "        \"Consumer consent provided?\", \n",
        "        \"Submitted via\", \n",
        "        \"Date sent to company\", \n",
        "        \"Company response to consumer\", \n",
        "        \"Timely response?\", \n",
        "        \"Consumer disputed?\", \n",
        "        \"Complaint ID\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "loan_complaint_data = loader.load()\n",
        "\n",
        "# Set page_content to the complaint narrative\n",
        "for doc in loan_complaint_data:\n",
        "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]\n",
        "\n",
        "print(f\"✅ Loaded {len(loan_complaint_data)} loan complaint documents\")\n",
        "print(f\"Example document preview: {loan_complaint_data[0].page_content[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Configure Synthetic Data Generation Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Synthetic data generation models configured successfully!\n"
          ]
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Set up the LLM and embedding models for synthetic data generation\n",
        "# Using the same models as in the reference notebooks\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
        "\n",
        "print(\"✅ Synthetic data generation models configured successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generate Synthetic Golden Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating synthetic dataset using RAGAS...\n",
            "Using first 50 documents from 825 total loan complaint documents\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(78857) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8b28f96728d412fabd339b2a28f2413",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/31 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python(78858) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c401d17af6f42ddb993620dbc3e1f87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node 6b8d73e1-29fa-4f07-9b88-b27064a9365e does not have a summary. Skipping filtering.\n",
            "Node a87fad26-84a6-439e-b694-093f6d4fd8d4 does not have a summary. Skipping filtering.\n",
            "Node f0aed855-110c-475a-be23-bc8a824282de does not have a summary. Skipping filtering.\n",
            "Node 7f504c21-fe02-4aa4-a883-132ac187c471 does not have a summary. Skipping filtering.\n",
            "Node dcbecf25-4519-45bc-a69d-74d9829bda6a does not have a summary. Skipping filtering.\n",
            "Node 270675d9-07ce-4d85-a1a1-771b6fa21730 does not have a summary. Skipping filtering.\n",
            "Node 7a2b2eca-ce3f-4965-897d-b5644af0c308 does not have a summary. Skipping filtering.\n",
            "Node 67becb17-310b-42d1-9df8-1d16312ca709 does not have a summary. Skipping filtering.\n",
            "Node 58bad9a5-4455-40b9-b2c4-7ec08bff9b8e does not have a summary. Skipping filtering.\n",
            "Node f1d56d9b-2e58-45ce-95e9-00bacb2c4189 does not have a summary. Skipping filtering.\n",
            "Node a223fb2b-7abe-4bad-8e3d-e1e1480563bf does not have a summary. Skipping filtering.\n",
            "Node 2b636fb4-ec7a-4508-9453-6e2f6fdecf51 does not have a summary. Skipping filtering.\n",
            "Node 7b4dabe6-7e2d-455a-a31c-a7cdafbf27f6 does not have a summary. Skipping filtering.\n",
            "Node e7b23a2a-cfd2-494f-a86c-cc67b46d61ec does not have a summary. Skipping filtering.\n",
            "Node 91e63743-0601-4163-932a-e89549317e97 does not have a summary. Skipping filtering.\n",
            "Node 591f4ca9-2753-4b8c-a782-dace6e05676a does not have a summary. Skipping filtering.\n",
            "Node 62c82f5a-23dd-4745-8639-b1d82a69e8bf does not have a summary. Skipping filtering.\n",
            "Node 2bdf515f-4964-4ec3-835a-7ec72e574f04 does not have a summary. Skipping filtering.\n",
            "Node dd50026d-d84b-4ca6-91b0-61cc3bb5a907 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "943ac74628a44978b6d652929686b93b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/131 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea6397798a8a4cd09fe4a745cac620e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86f26e34cfbb41dd941f1c8250704854",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcbaf144c74a4b44b3f14dd880f133d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e54597b181af410191e84d5466203b9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Generated 15 synthetic Q&A pairs successfully!\n"
          ]
        }
      ],
      "source": [
        "# Generate synthetic dataset using the abstracted SDG approach\n",
        "print(\"Generating synthetic dataset using RAGAS...\")\n",
        "print(f\"Using first 50 documents from {len(loan_complaint_data)} total loan complaint documents\")\n",
        "\n",
        "# Initialize the test set generator\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "\n",
        "# Generate 15 synthetic Q&A pairs (reduced from original 27)\n",
        "synthetic_dataset = generator.generate_with_langchain_docs(\n",
        "    loan_complaint_data[:50],  # Use first 50 docs to balance quality and speed\n",
        "    testset_size=15  # MODIFIED: Generate 15 Q&A pairs instead of 25/27\n",
        ")\n",
        "\n",
        "print(f\"✅ Generated {len(synthetic_dataset)} synthetic Q&A pairs successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Convert to Pandas and Display Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Golden dataset shape: (15, 4)\n",
            "Columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']\n",
            "\n",
            "================================================================================\n",
            "DATASET PREVIEW:\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is Nelnet?</td>\n",
              "      <td>[The federal student loan COVID-19 forbearance...</td>\n",
              "      <td>Nelnet is the servicer for federal student loa...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How is Aidvantage handling my IDR repayment am...</td>\n",
              "      <td>[I submitted my annual Income-Driven Repayment...</td>\n",
              "      <td>Aidvantage assigned me a repayment amount that...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does FERPA protect my personal and financi...</td>\n",
              "      <td>[My personal and financial data was compromise...</td>\n",
              "      <td>My personal and financial data was compromised...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0                                    What is Nelnet?   \n",
              "1  How is Aidvantage handling my IDR repayment am...   \n",
              "2  How does FERPA protect my personal and financi...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [The federal student loan COVID-19 forbearance...   \n",
              "1  [I submitted my annual Income-Driven Repayment...   \n",
              "2  [My personal and financial data was compromise...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Nelnet is the servicer for federal student loa...   \n",
              "1  Aidvantage assigned me a repayment amount that...   \n",
              "2  My personal and financial data was compromised...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "EXAMPLE QUESTION-ANSWER PAIR:\n",
            "================================================================================\n",
            "Question: What is Nelnet?\n",
            "\n",
            "Expected Answer: Nelnet is the servicer for federal student loans, and payments on these loans were not re-amortized until very recently after the end of the COVID-19 forbearance program. The new payment amount starting from the specified date will nearly double the previous payment, and re-amortization was expected to occur once the forbearance ended to help reduce the impact on borrowers.\n",
            "\n",
            "Reference Contexts (1 chunks):\n",
            "  Context 1: The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the synthetic dataset to pandas DataFrame\n",
        "golden_df = synthetic_dataset.to_pandas()\n",
        "\n",
        "print(f\"Golden dataset shape: {golden_df.shape}\")\n",
        "print(f\"Columns: {list(golden_df.columns)}\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET PREVIEW:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Display first few examples\n",
        "display(golden_df.head(3))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXAMPLE QUESTION-ANSWER PAIR:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Show a detailed example\n",
        "example_idx = 0\n",
        "print(f\"Question: {golden_df.iloc[example_idx]['user_input']}\")\n",
        "print(f\"\\nExpected Answer: {golden_df.iloc[example_idx]['reference']}\")\n",
        "print(f\"\\nReference Contexts ({len(golden_df.iloc[example_idx]['reference_contexts'])} chunks):\")\n",
        "for i, context in enumerate(golden_df.iloc[example_idx]['reference_contexts'][:2]):  # Show first 2 contexts\n",
        "    print(f\"  Context {i+1}: {context[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Upload Dataset to LangSmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created LangSmith dataset: loan-complaints-golden-dataset-standalone-20250725-105810\n",
            "Dataset ID: b7278dc2-7a24-4d9b-8e2e-8175adf6411d\n"
          ]
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize LangSmith client\n",
        "client = Client()\n",
        "\n",
        "# Create a unique dataset name with timestamp\n",
        "dataset_name = f\"loan-complaints-golden-dataset-standalone-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "\n",
        "# Create the dataset in LangSmith\n",
        "try:\n",
        "    langsmith_dataset = client.create_dataset(\n",
        "        dataset_name=dataset_name,\n",
        "        description=\"Standalone golden dataset for RAG evaluation using 15 synthetic loan complaint Q&A pairs generated with RAGAS\"\n",
        "    )\n",
        "    print(f\"✅ Created LangSmith dataset: {dataset_name}\")\n",
        "    print(f\"Dataset ID: {langsmith_dataset.id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating dataset: {e}\")\n",
        "    # If dataset already exists, get it\n",
        "    langsmith_dataset = client.read_dataset(dataset_name=dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading examples to LangSmith...\n",
            "✅ Successfully uploaded 15/15 examples to LangSmith dataset!\n"
          ]
        }
      ],
      "source": [
        "# Upload examples to LangSmith dataset\n",
        "print(\"Uploading examples to LangSmith...\")\n",
        "\n",
        "upload_count = 0\n",
        "for idx, row in golden_df.iterrows():\n",
        "    try:\n",
        "        client.create_example(\n",
        "            inputs={\n",
        "                \"question\": row[\"user_input\"]\n",
        "            },\n",
        "            outputs={\n",
        "                \"answer\": row[\"reference\"]\n",
        "            },\n",
        "            metadata={\n",
        "                \"reference_contexts\": row[\"reference_contexts\"],\n",
        "                \"synthesizer_name\": row.get(\"synthesizer_name\", \"unknown\"),\n",
        "                \"evolution_type\": row.get(\"evolution_type\", \"unknown\"),\n",
        "                \"episode_done\": row.get(\"episode_done\", False),\n",
        "                \"dataset_size\": len(golden_df),\n",
        "                \"source\": \"standalone_generator\"\n",
        "            },\n",
        "            dataset_id=langsmith_dataset.id\n",
        "        )\n",
        "        upload_count += 1\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading example {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"✅ Successfully uploaded {upload_count}/{len(golden_df)} examples to LangSmith dataset!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "GOLDEN DATASET GENERATION COMPLETE!\n",
            "================================================================================\n",
            "📊 Dataset Name: loan-complaints-golden-dataset-standalone-20250725-105810\n",
            "📊 Dataset ID: b7278dc2-7a24-4d9b-8e2e-8175adf6411d\n",
            "📊 Number of Q&A Pairs: 15\n",
            "📊 Source Documents Used: 825 (first 50 for generation)\n",
            "📊 Generation Method: RAGAS Abstracted SDG\n",
            "📊 Models Used:\n",
            "   - LLM: gpt-4.1-nano\n",
            "   - Embeddings: text-embedding-3-small\n",
            "\n",
            "🎯 READY FOR EVALUATION!\n",
            "This dataset can now be used to evaluate RAG chains with both LangSmith and RAGAS metrics.\n",
            "\n",
            "📋 Dataset Structure:\n",
            "   - Questions: 15 unique\n",
            "   - Answer Length: 606 chars avg\n",
            "   - Context Chunks: 1.7 per question\n",
            "\n",
            "📈 Synthesizer Distribution:\n",
            "   - single_hop_specifc_query_synthesizer: 5 questions\n",
            "   - multi_hop_abstract_query_synthesizer: 5 questions\n",
            "   - multi_hop_specific_query_synthesizer: 5 questions\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GOLDEN DATASET GENERATION COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"📊 Dataset Name: {dataset_name}\")\n",
        "print(f\"📊 Dataset ID: {langsmith_dataset.id}\")\n",
        "print(f\"📊 Number of Q&A Pairs: {len(golden_df)}\")\n",
        "print(f\"📊 Source Documents Used: {len(loan_complaint_data)} (first 50 for generation)\")\n",
        "print(f\"📊 Generation Method: RAGAS Abstracted SDG\")\n",
        "print(f\"📊 Models Used:\")\n",
        "print(f\"   - LLM: gpt-4.1-nano\")\n",
        "print(f\"   - Embeddings: text-embedding-3-small\")\n",
        "\n",
        "print(\"\\n🎯 READY FOR EVALUATION!\")\n",
        "print(\"This dataset can now be used to evaluate RAG chains with both LangSmith and RAGAS metrics.\")\n",
        "print(\"\\n📋 Dataset Structure:\")\n",
        "print(f\"   - Questions: {golden_df['user_input'].nunique()} unique\")\n",
        "print(f\"   - Answer Length: {golden_df['reference'].str.len().mean():.0f} chars avg\")\n",
        "print(f\"   - Context Chunks: {golden_df['reference_contexts'].apply(len).mean():.1f} per question\")\n",
        "\n",
        "# Display synthesizer distribution\n",
        "synthesizer_counts = golden_df['synthesizer_name'].value_counts()\n",
        "print(f\"\\n📈 Synthesizer Distribution:\")\n",
        "for synthesizer, count in synthesizer_counts.items():\n",
        "    print(f\"   - {synthesizer}: {count} questions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Save Dataset Locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 Dataset saved locally as: golden_dataset_20250725_105853.csv\n",
            "💾 Dataset saved locally as JSON: golden_dataset_20250725_105853.json\n"
          ]
        }
      ],
      "source": [
        "# Save the dataset locally for backup/reference\n",
        "local_filename = f\"golden_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "golden_df.to_csv(local_filename, index=False)\n",
        "print(f\"💾 Dataset saved locally as: {local_filename}\")\n",
        "\n",
        "# Also save as JSON for better context preservation\n",
        "json_filename = f\"golden_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "golden_df.to_json(json_filename, orient='records', indent=2)\n",
        "print(f\"💾 Dataset saved locally as JSON: {json_filename}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
